{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba3c43b4404c447cbb4b5a8e2d5b280a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11f8481455de483dbfbc2a2ad1b777b8",
              "IPY_MODEL_90946b9d5a32417faeebd6ab774f23e4",
              "IPY_MODEL_ee1b5eb3fb83486e9de62f78b9726b19"
            ],
            "layout": "IPY_MODEL_81e8ee453f404301b7c3bce9b227b85a"
          }
        },
        "11f8481455de483dbfbc2a2ad1b777b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd50723adba8494092f5fb54d3eff9bf",
            "placeholder": "​",
            "style": "IPY_MODEL_5a86a4a7b4b648c89b59707ce578eb2e",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "90946b9d5a32417faeebd6ab774f23e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0567ebb14fe3427dafc7f3264dffe7ce",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09ffb6e1f8064457b1c44abbd75780f5",
            "value": 12
          }
        },
        "ee1b5eb3fb83486e9de62f78b9726b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccac5dd8b69c4d1aa8b9a058d9e40481",
            "placeholder": "​",
            "style": "IPY_MODEL_9beee3fe011245f696fa6a4012c1cfa8",
            "value": " 12/12 [00:02&lt;00:00,  5.26 examples/s]"
          }
        },
        "81e8ee453f404301b7c3bce9b227b85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd50723adba8494092f5fb54d3eff9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a86a4a7b4b648c89b59707ce578eb2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0567ebb14fe3427dafc7f3264dffe7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ffb6e1f8064457b1c44abbd75780f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccac5dd8b69c4d1aa8b9a058d9e40481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9beee3fe011245f696fa6a4012c1cfa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTlImN1MHbCD",
        "outputId": "c55e30be-58bd-4fc9-dba8-1db8d96d60af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.12.12)\n",
            "Requirement already satisfied: unsloth_zoo>=2024.12.7 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2024.12.7)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.0.29.post1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.0)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.9.5)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.47.1)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.27.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.12.7->unsloth) (24.12.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.12.7->unsloth) (11.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install unsloth\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandbの設定"
      ],
      "metadata": {
        "id": "_zPwmdoZxVf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "!wandb login $wandb_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAEKQbDVxU-w",
        "outputId": "6bd91edd-ac2f-4230-cfb2-c542ace77461"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの準備"
      ],
      "metadata": {
        "id": "_xSWmRFSxXTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの準備\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# 基本設定\n",
        "max_seq_length = 2048  # RoPEスケーリングで自動調整\n",
        "dtype = None  # GPU種別に応じて自動選択\n",
        "load_in_4bit = True  # メモリ効率化のための4bit量子化\n",
        "\n",
        "\n",
        "# モデルとトークナイザーの読み込み\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhjL1OTuHfr8",
        "outputId": "b29b72dd-fa63-4765-ac03-120f26272752"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ファインチューニングする前に質問してみる。"
      ],
      "metadata": {
        "id": "irUI1amgcTjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 高速推論の有効化\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"フィボナッチ数列を続けてください: 1, 1, 2, 3, 5, 8,\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "llm_answer = tokenizer.decode(\n",
        "    outputs[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPzRwOScXDC",
        "outputId": "1a29a9fd-8989-4b49-8f3d-332d99195852"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 01 Jan 2025\n",
            "\n",
            "user\n",
            "\n",
            "フィボナッチ数列を続けてください: 1, 1, 2, 3, 5, 8,assistant\n",
            "\n",
            "フィボナッチ数列は、各項が前の 2 項の合計として得られる数列です。次の項は次のようになります。\n",
            "\n",
            "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,...\n",
            "\n",
            "フィボナッチ数列は、自然数の素数の分布や、自然数の平方数の数の分布など、さまざまな数学的問題に使用されることが多いです。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAアダプターの追加\n",
        "\n",
        "効率的にチューニングするためにLoRAアダプターで学習する。"
      ],
      "metadata": {
        "id": "A25jdSQAhMfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRAアダプターの追加\n",
        "# パラメータの効率的な更新のため、LoRAアダプターを追加します。\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # LoRAアダプターのランク（推奨値: 8, 16, 32, 64, 128）\n",
        "    target_modules = [ # ファインチューニングするすべてのモジュールを選択します。メモリ使用量を削減してトレーニングを高速化するために、いくつか削除することもできますが、お勧めしません。すべてのモジュールでトレーニングしてください。\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 16, # ファインチューニングのスケーリング係数。数値が大きいほど、ファインチューニングはデータセットについて多く学習しますが、オーバーフィッティングを促進する可能性があります。ランクrと同じか、2倍にすることをお勧めします。\n",
        "    lora_dropout = 0, # トレーニングを高速化するために、これを0のままにします。オーバーフィッティングを減らすことはできますが、それほどではありません。\n",
        "    bias = \"none\", # トレーニングを高速化し、オーバーフィッティングを減らすために、これを0のままにします。\n",
        "    use_gradient_checkpointing = \"unsloth\", # オプションは、True、False、\"unsloth\"です。\"unsloth\"を推奨します。これは、メモリ使用量をさらに30%削減し、非常に長いコンテキストのファインチューニングをサポートしているためです。詳細については、https://unsloth.ai/blog/long-context をご覧ください。\n",
        "    random_state = 3407,\n",
        "    use_rslora=False,  # ランク安定化LoRAをサポート。lora_alpha = 16を自動的に設定する高度な機能です。必要に応じて使用できます。\n",
        "    loftq_config = None,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvhch4XQHoGi",
        "outputId": "5794b3b4-268c-4bb2-84f6-36aff57b682c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの準備"
      ],
      "metadata": {
        "id": "OSNIEaW-h5Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意：ファインチューニングするには`dataset`形式になっている必要がある。"
      ],
      "metadata": {
        "id": "8uvz36Hvykhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/the-exile-110/unsloth-llama3-fine-tuning/blob/main/dataset.json\n",
        "json_data =[\n",
        "  {\n",
        "    \"instruction\": \"心臓が一つだけ残っても生きられますか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"はい、人間はもともと心臓が一つしかありません。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"もうすぐ水泳の授業があるのに、昨日洗った水着がまだ乾いていません。どうすればいいですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"水泳中はどうせ水着が濡れるので、乾かす必要はありません。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"一度しか生まれていないのに、なぜ毎年誕生日を祝うのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"誕生日を祝うのは、生まれた日を祝うのではなく、毎年の成長と経験を記念し、未来の一年に期待と祝福を込めるためです。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"私は25歳で父は50歳です。つまり、父は2歳で私を生んだのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"違います。あなたが25歳で、父が50歳なら、父はあなたが生まれたとき25歳だったということです。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"5000円で新車を買うなら何がいいですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"どうしても答えるなら、おもちゃの車しか買えないでしょう。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"酸素と水素を同時に吸入したら、水を飲んでいることになるのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"違います。水は水素と酸素で構成されていますが、単に酸素と水素を吸入するだけでは水は形成されません。水の形成には特定の条件下で化学反応が必要で、その反応は激しく、大量の熱を発生します。人体にとって非常に危険ですので、酸素と水素を吸入して水を飲むことはできません。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"赤壁の戦いで、曹操はなぜ119に電話して救助を求めなかったのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"三国時代には電話や現代の消防システムがなかったので、曹操は119に電話して救助を求めることはできませんでした。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"一昨日は誕生日おめでとうと言われたのに、今日はなぜ言われないのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"今日はあなたの誕生日ではなく、一昨日があなたの誕生日だったからです。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Bluetoothイヤホンが壊れたのですが、病院の歯科か耳科に行くべきですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Bluetoothイヤホンが壊れた場合、修理店や専門の電子製品修理技術者に頼むべきで、病院の歯科や耳科に行くべきではありません。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"1キロの綿と1キロの鉄が同時に水に落ちたら、どちらを先に救いますか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"実際には、鉄は水に沈み、綿は水面に浮くので、救う必要はありません。どちらも救う必要はありません。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"刑務所には全て罪人がいるのに、なぜ警察は刑務所に行って捕まえないのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"彼らはすでに捕まって刑務所にいるので、再び捕まえる必要はありません。\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"人間には左耳と右耳しかないのに、なぜ中耳炎になるのですか？\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"中耳炎は「真ん中の耳」に発生するのではなく、耳の構造の中耳部分に発生する炎症です。耳の構造は外耳、中耳、内耳の三つに分かれており、中耳炎は中耳部分が炎症を起こすことです。つまり、人間には左耳と右耳しかなくても中耳炎になることがあります。\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "werIX708H38H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファインチューニングする時用にdatasetを作成する\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "# ファインチューニング用にテンプレートプロンプトを用意する\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "def formatting_prompt(examples: list) -> dict:\n",
        "    instructions = [item[\"instruction\"] for item in examples]\n",
        "    inputs = [item[\"input\"] for item in examples]\n",
        "    outputs = [item[\"output\"] for item in examples]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "_dataset = formatting_prompt(json_data)\n",
        "# datasetの型\n",
        "dataset = Dataset.from_dict(_dataset)"
      ],
      "metadata": {
        "id": "u2Qpvs0ZoQSN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rWyasXriieN",
        "outputId": "04eecee5-1082-4225-e5f3-c38e0ac48e69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n心臓が一つだけ残っても生きられますか？\\n### Input:\\n\\n### Response:\\nはい、人間はもともと心臓が一つしかありません。'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの学習"
      ],
      "metadata": {
        "id": "TYyFUZ0Eh874"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # 短いシーケンスの場合、トレーニングを5倍高速化できます\n",
        "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,# GPUのメモリをより多く活用したい場合は、バッチサイズを増やします。また、トレーニングをよりスムーズにし、オーバーフィットを防ぐためにも、これを増やします。ただし、パディングの問題によりトレーニングが実際に遅くなる可能性があるため、通常はお勧めしません。代わりに、gradient_accumulation_stepsを増やすことをお勧めします。\n",
        "        gradient_accumulation_steps = 4,#上記のバッチサイズを増やすことと同じ効果がありますが、メモリ消費量には影響しません。通常、トレーニング損失曲線をよりスムーズにしたい場合は、これを増やすことをお勧めします。\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # トレーニングを高速化するために、ステップを60に設定しました。数時間かかる可能性のある完全なトレーニング実行の場合は、代わりにmax_stepsをコメントアウトし、num_train_epochs = 1に置き換えます。1に設定すると、データセット全体を1回完全にパスします。通常は1〜3パスを推奨します。それ以上は推奨しません。それ以上パスするとオーバーフィットの可能性が高くなります。\n",
        "        #num_train_epochs=1\n",
        "        learning_rate = 2e-4, # ファインチューニングプロセスを遅くしたい場合は学習率を下げますが、より高い精度の結果に収束する可能性があります。通常は、試してみる値として、2e-4、1e-4、5e-5、2e-5を推奨します。\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 学習の実行\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba3c43b4404c447cbb4b5a8e2d5b280a",
            "11f8481455de483dbfbc2a2ad1b777b8",
            "90946b9d5a32417faeebd6ab774f23e4",
            "ee1b5eb3fb83486e9de62f78b9726b19",
            "81e8ee453f404301b7c3bce9b227b85a",
            "cd50723adba8494092f5fb54d3eff9bf",
            "5a86a4a7b4b648c89b59707ce578eb2e",
            "0567ebb14fe3427dafc7f3264dffe7ce",
            "09ffb6e1f8064457b1c44abbd75780f5",
            "ccac5dd8b69c4d1aa8b9a058d9e40481",
            "9beee3fe011245f696fa6a4012c1cfa8"
          ]
        },
        "id": "oSzlqxPFiBsD",
        "outputId": "4cec66ef-a67b-44bf-ddc9-41ce70437b17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/12 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba3c43b4404c447cbb4b5a8e2d5b280a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 12 | Num Epochs = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 02:04, Epoch 30/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.858900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.192700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.877100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.103700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.776300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.311100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.124900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.372400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.139100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.018900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.741100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.872400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.631700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.524700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.502100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.391100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.324900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.285300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.254200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.230400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.210800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.181700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.170700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.153000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.144900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.141800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.137300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.113200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.143400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.103000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.097500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.093800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.088900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.077800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.077800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.080400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.083600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.066800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.071900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.065100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.063600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.056400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.075200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.060100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.057600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習したモデルを使って推論させる"
      ],
      "metadata": {
        "id": "7wmzlT8By2Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 高速推論の有効化\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"フィボナッチ数列を続けてください: 1, 1, 2, 3, 5, 8,\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "llm_answer = tokenizer.decode(\n",
        "    outputs[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "id": "gEs-kUGxH4BA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4b6bc5-2253-4677-e0de-5722e05e15ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 01 Jan 2025\n",
            "\n",
            "user\n",
            "\n",
            "フィボナッチ数列を続けてください: 1, 1, 2, 3, 5, 8,assistant\n",
            "\n",
            "次の数は8と言われていますが、実際には次の数は8と言われないで、次の数は8と同時に現れるのではなく、次の数は8と同時に現れるのではなく、次の数は8と同時に現れるのではなく、次の数は8の次に現れる数で、実際には9です。\n",
            "\n",
            "つまり、正しく言えば、1、1、2、3、5、8、9です。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 高速推論の有効化\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"赤壁の戦いで、曹操はなぜ119に電話して救助を求めなかったのですか？\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "llm_answer = tokenizer.decode(\n",
        "    outputs[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISbX-ufJQtjC",
        "outputId": "40326dda-96bd-43e4-d38d-c44da5f90520"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 01 Jan 2025\n",
            "\n",
            "user\n",
            "\n",
            "赤壁の戦いで、曹操はなぜ119に電話して救助を求めなかったのですか？assistant\n",
            "\n",
            "曹操は119に電話して救助を求めることはできませんでした。119は現代の消防システムであり、119は119に電話して消防を求めることです。曹操は119に電話して救助を求めることはできませんでしたので、赤壁の戦いでどうしたらいいですか？\n",
            "\n",
            "実際には、曹操は119に電話して救助を求めるのではなく、現代の消防システムがなかったので、救助を求めるにはどうしたらいいですか？\n",
            "\n",
            "曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めることです。つまり、曹操は119に電話して救助を求めるのではなく、119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "実際には、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めることです。つまり、曹操は119に電話して救助を求めるのではなく、119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119に電話して消防を求めるべきで、119に電話して救助を求めるべきではありません。\n",
            "\n",
            "つまり、曹操は119に電話して救助を求めるのではなく、119は119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAモデルを保存する"
      ],
      "metadata": {
        "id": "mZMStRo51BKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "IH7iMvVMH4JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21acdaa-b646-42fb-fc89-35fc8dbcb6ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルを結合し、GGUF量子化して保存する"
      ],
      "metadata": {
        "id": "_DRnDvIH1F86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\n",
        "    \"model\",\n",
        "    tokenizer,\n",
        "    quantization_method = [\n",
        "        \"q4_k_m\",\n",
        "        \"q8_0\",\n",
        "        \"q5_k_m\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "uAN4Ru4yH4Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcbfccf-80dd-44a5-937c-4670a3320741"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Will remove a cached repo with size 1.4K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.34 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28/28 [00:01<00:00, 22.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at model into f16 GGUF format.\n",
            "The output location will be /content/model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 24\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-01-01 12:58:55.295776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-01 12:58:55.333674: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-01 12:58:55.340968: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-01 12:58:57.537822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- if strftime_now is defined %}\n",
            "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "    {%- else %}\n",
            "        {%- set date_string = \"26 Jul 2024\" %}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "        {{- '\"parameters\": ' }}\n",
            "        {{- tool_call.arguments | tojson }}\n",
            "        {{- \"}\" }}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
            "Writing: 100%|██████████| 6.43G/6.43G [01:33<00:00, 69.0Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4404 (0827b2c1)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  1918.35 MB\n",
            "\n",
            "main: quantize time = 342033.74 ms\n",
            "main:    total time = 342033.74 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
            "main: build = 4404 (0827b2c1)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q8_0.gguf' as Q8_0 using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q8_0 .. size =   751.50 MiB ->   399.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  3255.90 MB\n",
            "\n",
            "main: quantize time = 80100.95 ms\n",
            "main:    total time = 80100.95 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q8_0.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
            "main: build = 4404 (0827b2c1)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q5_K_M.gguf' as Q5_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  2207.10 MB\n",
            "\n",
            "main: quantize time = 302381.78 ms\n",
            "main:    total time = 302381.78 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q5_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/_unsloth_sentencepiece_temp.zip /content/_unsloth_sentencepiece_temp\n",
        "!zip -r /content/huggingface_tokenizers_cache.zip /content/huggingface_tokenizers_cache\n",
        "!zip -r /content/llama.cpp.zip /content/llama.cpp\n",
        "!zip -r /content/lora_model.zip /content/lora_model\n",
        "!zip -r /content/model.zip /content/model\n",
        "!zip -r /content/outputs.zip /content/outputs\n",
        "!zip -r /content/wandb.zip /content/wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaQCQGYwQIgd",
        "outputId": "2e5c57d4-c4aa-41b8-ff60-76397afdd80d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/_unsloth_sentencepiece_temp/ (stored 0%)\n",
            "  adding: content/_unsloth_sentencepiece_temp/unsloth/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/5a1d225b3b447247fd733c526e0e2180016e8642/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/5a1d225b3b447247fd733c526e0e2180016e8642/tokenizer.json (deflated 74%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/5a1d225b3b447247fd733c526e0e2180016e8642/tokenizer_config.json (deflated 94%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/snapshots/5a1d225b3b447247fd733c526e0e2180016e8642/special_tokens_map.json (deflated 71%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/refs/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/refs/main (deflated 3%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/.no_exist/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/.no_exist/5a1d225b3b447247fd733c526e0e2180016e8642/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/.no_exist/5a1d225b3b447247fd733c526e0e2180016e8642/tokenizer.model (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/.no_exist/5a1d225b3b447247fd733c526e0e2180016e8642/chat_template.jinja (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/.no_exist/5a1d225b3b447247fd733c526e0e2180016e8642/added_tokens.json (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/blobs/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/blobs/3c1d04911c269b925af977a3151c9704e990e4d0 (deflated 71%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/blobs/d06e02887aecbff6ebd02feddfd72f4875dff0a9 (deflated 94%)\n",
            "  adding: content/huggingface_tokenizers_cache/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/blobs/79e3e522635f3171300913bb421464a87de6222182a0570b9b2ccba2a964b2b4 (deflated 74%)\n",
            "  adding: content/huggingface_tokenizers_cache/.locks/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/.locks/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/ (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/.locks/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/d06e02887aecbff6ebd02feddfd72f4875dff0a9.lock (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/.locks/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/79e3e522635f3171300913bb421464a87de6222182a0570b9b2ccba2a964b2b4.lock (stored 0%)\n",
            "  adding: content/huggingface_tokenizers_cache/.locks/models--unsloth--llama-3.2-3b-instruct-bnb-4bit/3c1d04911c269b925af977a3151c9704e990e4d0.lock (stored 0%)\n",
            "  adding: content/llama.cpp/ (stored 0%)\n",
            "  adding: content/llama.cpp/ci/ (stored 0%)\n",
            "  adding: content/llama.cpp/ci/run.sh (deflated 89%)\n",
            "  adding: content/llama.cpp/ci/README.md (deflated 48%)\n",
            "  adding: content/llama.cpp/requirements.txt (deflated 60%)\n",
            "  adding: content/llama.cpp/media/ (stored 0%)\n",
            "  adding: content/llama.cpp/media/matmul.svg (deflated 93%)\n",
            "  adding: content/llama.cpp/media/llama-leader.jpeg (deflated 0%)\n",
            "  adding: content/llama.cpp/media/llama0-banner.png (deflated 8%)\n",
            "  adding: content/llama.cpp/media/llama1-logo.png (deflated 18%)\n",
            "  adding: content/llama.cpp/media/llama0-logo.png (deflated 2%)\n",
            "  adding: content/llama.cpp/media/matmul.png (deflated 15%)\n",
            "  adding: content/llama.cpp/media/llama1-banner.png (deflated 21%)\n",
            "  adding: content/llama.cpp/convert_llama_ggml_to_gguf.py (deflated 72%)\n",
            "  adding: content/llama.cpp/include/ (stored 0%)\n",
            "  adding: content/llama.cpp/include/llama.h (deflated 77%)\n",
            "  adding: content/llama.cpp/include/llama-cpp.h (deflated 63%)\n",
            "  adding: content/llama.cpp/.pre-commit-config.yaml (deflated 47%)\n",
            "  adding: content/llama.cpp/flake.nix (deflated 63%)\n",
            "  adding: content/llama.cpp/models/ (stored 0%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-phi-3.gguf.out (deflated 76%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-baichuan.gguf (deflated 74%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-aquila.gguf (deflated 72%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-mpt.gguf.out (deflated 68%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-chameleon.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.out (deflated 72%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-phi-3.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.out (deflated 71%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-falcon.gguf (deflated 70%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-starcoder.gguf.out (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-phi-3.gguf (deflated 74%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-chameleon.gguf.out (deflated 78%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-llama-spm.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-qwen2.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/.editorconfig (stored 0%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf (deflated 70%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-llama-spm.gguf.out (deflated 76%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-command-r.gguf.out (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-command-r.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-starcoder.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-falcon.gguf.out (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-qwen2.gguf.out (deflated 67%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-mpt.gguf (deflated 70%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.out (deflated 70%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-gpt-neox.gguf (deflated 70%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-gpt-2.gguf (deflated 71%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-gpt-2.gguf.out (deflated 71%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-bert-bge.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-llama-spm.gguf (deflated 74%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-falcon.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.out (deflated 67%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-refact.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-bert-bge.gguf.out (deflated 74%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-mpt.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-starcoder.gguf (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-refact.gguf (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-refact.gguf.out (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-command-r.gguf (deflated 67%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-bert-bge.gguf (deflated 77%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-gpt-2.gguf.inp (deflated 69%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-llama-bpe.gguf (deflated 72%)\n",
            "  adding: content/llama.cpp/models/ggml-vocab-qwen2.gguf (deflated 71%)\n",
            "  adding: content/llama.cpp/pyproject.toml (deflated 48%)\n",
            "  adding: content/llama.cpp/convert_hf_to_gguf.py (deflated 82%)\n",
            "  adding: content/llama.cpp/.gitmodules (deflated 21%)\n",
            "  adding: content/llama.cpp/CMakeLists.txt (deflated 67%)\n",
            "  adding: content/llama.cpp/CMakePresets.json (deflated 84%)\n",
            "  adding: content/llama.cpp/.flake8 (deflated 43%)\n",
            "  adding: content/llama.cpp/CODEOWNERS (deflated 25%)\n",
            "  adding: content/llama.cpp/Makefile (deflated 78%)\n",
            "  adding: content/llama.cpp/.gitignore (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/include/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-vulkan.h (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-cpp.h (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-kompute.h (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-backend.h (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-opencl.h (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-cann.h (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-cpu.h (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-alloc.h (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-cuda.h (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml.h (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-metal.h (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-rpc.h (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-sycl.h (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-opt.h (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/include/ggml-blas.h (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/CMakeLists.txt (deflated 71%)\n",
            "  adding: content/llama.cpp/ggml/.gitignore (deflated 36%)\n",
            "  adding: content/llama.cpp/ggml/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml.c (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-quants.h (deflated 90%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-alloc.c (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec.comp (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.comp (deflated 70%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pool2d.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/silu.comp (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q3_k.comp (deflated 76%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm.comp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_split_k_reduce.comp (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_1.comp (deflated 54%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_unary_head.comp (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/relu.comp (deflated 40%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat2_support.comp (deflated 3%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/acc.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu.comp (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/clamp.comp (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/tanh.comp (deflated 40%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu_quick.comp (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/diag_mask_inf.comp (deflated 48%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_p021.comp (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_head.comp (deflated 24%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_f32.comp (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/group_norm.comp (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/contig_copy.comp (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pad.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/add.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/soft_max.comp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/concat.comp (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_head.comp (deflated 35%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.comp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_nc.comp (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_head.comp (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows.comp (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q6_k.comp (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q6_k.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul.comp (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/wkv6.comp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q8_0.comp (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy.comp (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows_quant.comp (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/types.comp (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_0.comp (deflated 51%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/square.comp (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/flash_attn_cm2.comp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/norm.comp (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/timestep_embedding.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_cm2.comp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_0.comp (deflated 54%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/cos.comp (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sum_rows.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/div.comp (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/scale.comp (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_1.comp (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/leaky_relu.comp (deflated 42%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argsort.comp (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_nl.comp (deflated 51%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.comp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/im2col.comp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q2_k.comp (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q3_k.comp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sin.comp (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/ggml-vulkan.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-blas/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-blas/CMakeLists.txt (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-blas/ggml-blas.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cuh (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mmq.cu (deflated 76%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/vecdotq.cuh (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/binbcast.cuh (deflated 76%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/softmax.cuh (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/rope.cuh (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mma.cuh (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/sum.cuh (deflated 32%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/arange.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt (deflated 70%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/scale.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/argsort.cuh (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/pool2d.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/softmax.cu (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn.cu (deflated 89%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/count-equal.cu (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/common.cuh (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/binbcast.cu (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/diagmask.cuh (deflated 9%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/clamp.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/norm.cu (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/upscale.cu (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/argmax.cuh (deflated 13%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/concat.cuh (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/arange.cu (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/sumrows.cu (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/out-prod.cuh (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/diagmask.cu (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/norm.cuh (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/argmax.cu (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cuh (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/dequantize.cuh (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/scale.cu (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/wkv6.cu (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/quantize.cu (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/tsembd.cuh (deflated 10%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/convert.cuh (deflated 42%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f16.cuh (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mmvq.cu (deflated 87%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cuh (deflated 40%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/convert.cu (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/sum.cu (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/clamp.cu (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cuh (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/out-prod.cu (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cuh (deflated 10%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb16.cu (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb8.cu (deflated 42%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/generate_cu_files.py (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb16.cu (deflated 54%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqhalf-cpb32.cu (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-wmma-f16-instance-kqfloat-cpb32.cu (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu (deflated 14%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu (deflated 16%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn.cuh (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/unary.cuh (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/pad.cu (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cuh (deflated 9%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f32.cuh (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/getrows.cu (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/count-equal.cuh (deflated 9%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/tsembd.cu (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/sumrows.cuh (deflated 33%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/unary.cu (deflated 91%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mmvq.cuh (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/concat.cu (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/quantize.cuh (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/acc.cuh (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mmv.cuh (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/im2col.cu (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/im2col.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/argsort.cu (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/pool2d.cu (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/wkv6.cuh (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/cpy.cu (deflated 87%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/acc.cu (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mmq.cuh (deflated 88%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/vendors/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/vendors/musa.h (deflated 76%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/vendors/cuda.h (deflated 49%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/vendors/hip.h (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-common.cuh (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/mmv.cu (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/upscale.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/getrows.cuh (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/cpy.cuh (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/pad.cuh (deflated 8%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cuda/rope.cu (deflated 88%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-common.h (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-rpc/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-rpc/CMakeLists.txt (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp (deflated 82%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-quants.c (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/CMakeLists.txt (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/VERSION (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/setup.py (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/CMakeLists.txt (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/kompute-config.cmake (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/CNAME (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docker-builders/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docker-builders/KomputeBuilder.Dockerfile (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docker-builders/VulkanSDK.Dockerfile (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docker-builders/Makefile (deflated 71%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docker-builders/Swiftshader.Dockerfile (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/Makefile (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.gitignore (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/LICENSE (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/CONTRIBUTING.md (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.github/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.github/workflows/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.github/workflows/cpp_examples.yml (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.github/workflows/python_tests.yml (deflated 46%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.github/workflows/cpp_tests.yml (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/run_vgg7.py (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/.gitignore (deflated 4%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/sh_conv.py (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/sh_common.py (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/README.md (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/import_vgg7.py (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/w2wbinit.png (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/neural_network_vgg7/out.png (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/assets/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/assets/default_env.tres (deflated 26%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/assets/icon.png.import (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/assets/DynamicExampleScene.tscn (deflated 48%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/assets/CustomModuleExampleScene.tscn (deflated 33%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/assets/icon.png (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/scripts/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/scripts/KomputeNativeLibrary.gdnlib (deflated 24%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/scripts/KomputeNativeClass.gdns (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/scripts/DynamicExampleScript.gd (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_resources/scripts/CustomModuleExampleScene.gd (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/.gitignore (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/README.md (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/README.md (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/SCsub (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/include/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/include/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/KomputeSummatorNode.h (deflated 51%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/register_types.cpp (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/config.py (deflated 12%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/register_types.h (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/custom_module/kompute_summator/KomputeSummatorNode.cpp (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_engine/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/godot_engine/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/project.godot (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/CMakeLists.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/README.md (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/src/KomputeSummator.hpp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/src/KomputeSummator.cpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/src/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_examples/gdnative_shared/src/KomputeGdNative.cpp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/CMakeLists.txt (deflated 49%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/README.md (deflated 49%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/src/CMakeLists.txt (deflated 26%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/src/main.cpp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/shader/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/shader/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/logistic_regression/shader/my_shader.comp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/CMakeLists.txt (deflated 49%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/README.md (deflated 48%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/src/CMakeLists.txt (deflated 21%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/src/main.cpp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/shader/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/shader/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/array_multiplication/shader/my_shader.comp (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/imp2_tiled.py (deflated 70%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/matmul_plot.py (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/README.md (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/imp1_naive.py (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/imp3_better_tiling.py (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/first_example.py (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/python_naive_matmul/benchmark.py (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/default_env.tres (deflated 26%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/TextFormat.theme (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/icon.png.import (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/DynamicExampleScene.tscn (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/CustomModuleExampleScene.tscn (deflated 34%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/icon.png (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/assets/roboto.ttf (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/scripts/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/scripts/KomputeNativeLibrary.gdnlib (deflated 24%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/scripts/KomputeNativeClass.gdns (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/scripts/DynamicExampleScript.gd (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_resources/scripts/CustomModuleExampleScene.gd (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/.gitignore (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/README.md (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/KomputeModelMLNode.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/SCsub (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/include/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/include/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/KomputeModelMLNode.h (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/register_types.cpp (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/config.py (deflated 12%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/kompute_model_ml/register_types.h (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/README.md (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/custom_module/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_engine/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/godot_engine/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/project.godot (deflated 46%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/CMakeLists.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/README.md (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/src/KomputeModelML.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/src/.gdignore (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/src/KomputeModelML.hpp (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/godot_logistic_regression/gdnative_shared/src/KomputeGdNative.cpp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/pi4_mesa_build/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/pi4_mesa_build/README.md (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/gradle.properties (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/.gitignore (deflated 7%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/gradlew (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/settings.gradle (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/java/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/java/com/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/java/com/ethicalml/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/java/com/ethicalml/kompute/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/java/com/ethicalml/kompute/KomputeJni.kt (deflated 70%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/assets/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/assets/kompute.jpg (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/assets/komputer-2.gif (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-anydpi-v26/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-anydpi-v26/kompute_icon.xml (deflated 40%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/layout/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/layout/activity_kompute_jni.xml (deflated 90%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values-w820dp/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values-w820dp/dimens.xml (deflated 32%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xhdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xhdpi/kompute_icon.png (deflated 4%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xhdpi/kompute_icon_foreground.png (deflated 4%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xhdpi/ic_launcher.png (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xxxhdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xxxhdpi/ic_launcher.png (deflated 1%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-hdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-hdpi/ic_launcher.png (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values/dimens.xml (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values/styles.xml (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values/strings.xml (deflated 21%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values/kompute_icon_background.xml (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/values/colors.xml (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xxhdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-xxhdpi/ic_launcher.png (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-mdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/res/mipmap-mdpi/ic_launcher.png (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/AndroidManifest.xml (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/CMakeLists.txt (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/KomputeModelML.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/KomputeJniNative.cpp (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/shader/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/shader/CMakeLists.txt (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/shader/my_shader.comp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/src/main/cpp/KomputeModelML.hpp (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/build.gradle (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/app/proguard-rules.pro (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/README.md (deflated 48%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/gradlew.bat (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/gradle/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/gradle/wrapper/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/gradle/wrapper/gradle-wrapper.properties (deflated 34%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/examples/android/android-simple/build.gradle (deflated 46%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/README.md (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.git (deflated 17%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.clang-format (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/external/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/external/bin/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/external/bin/xxd.c (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/CMakeLists.txt (deflated 71%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/Manager.hpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/Sequence.hpp (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/Kompute.hpp (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpMemoryBarrier.hpp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpTensorSyncDevice.hpp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpTensorFill.hpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpBase.hpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpAlgoDispatch.hpp (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpTensorSyncLocal.hpp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpBufferSyncLocal.hpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpMult.hpp (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpBufferSyncDevice.hpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/operations/OpTensorCopy.hpp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/logger/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/logger/Logger.hpp (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/Core.hpp (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/Tensor.hpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/include/kompute/Algorithm.hpp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpBufferSyncDevice.cpp (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/Algorithm.cpp (deflated 82%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/Tensor.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/CMakeLists.txt (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpMemoryBarrier.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/Sequence.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpTensorCopy.cpp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/Core.cpp (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpTensorSyncLocal.cpp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/Manager.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpBufferSyncLocal.cpp (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/CMakeLists.txt (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/glsl/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/glsl/ShaderOpMult.hpp.in (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/glsl/CMakeLists.txt (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/glsl/ShaderLogisticRegression.hpp.in (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/glsl/ShaderOpMult.comp (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/glsl/ShaderLogisticRegression.comp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/hlsl/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/shaders/hlsl/computeheadless.comp (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpTensorSyncDevice.cpp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpTensorFill.cpp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/logger/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/logger/CMakeLists.txt (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/logger/Logger.cpp (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/src/OpAlgoDispatch.cpp (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/pylintrc (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/MANIFEST.in (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/CODE_OF_CONDUCT.md (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/config/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/config/FindSphinx.cmake (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/vulkan_shader_compiler.cmake (deflated 71%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/code_coverage.cmake (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/deprecation_warnings.cmake (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/bin2h.cmake (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/komputeConfig.cmake.in (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/bin_file_to_header.cmake (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/cmake/check_vulkan_version.cmake (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/CMakeLists.txt (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/assets/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/assets/gcov.css (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/assets/custom.css (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/Makefile (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/genindex.rst (deflated 15%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/convolutional-net.rst (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/game-engine-godot.rst (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/raspberry-pi.rst (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/build-system.rst (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/custom-operations.rst (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/matmul-benchmark.rst (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/async-parallel.rst (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/reference.rst (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/variable-types.rst (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/advanced-examples.rst (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/python-reference.rst (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/mobile-android.rst (deflated 12%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/shaders-to-headers.rst (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/python-package.rst (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/community.rst (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/ci-tests.rst (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/python-examples.rst (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/overview/memory-management.rst (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/make.bat (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/conf.py (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/android-kompute.jpg (deflated 21%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-architecture.jpg (deflated 27%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/binder-cpp.jpg (deflated 31%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/queue-allocation.jpg (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-opmult.jpg (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute.jpg (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/komputer-2.gif (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-cpp-video.png (deflated 2%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-algorithm.jpg (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/komputer-logos.gif (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-python-video.png (deflated 1%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-operations.jpg (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-tensor.jpg (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-opcreatetensor.jpg (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture.jpg (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/logistic-regression.jpg (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-manager.jpg (deflated 54%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/binder-python.jpg (deflated 28%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/kompute-vulkan-architecture-sequence.jpg (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/komputer-godot-4.gif (deflated 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/android-editor.jpg (deflated 30%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/images/suspicious.jfif (deflated 10%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/index.rst (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/docs/Doxyfile.in (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/CMakeLists.txt (deflated 22%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/README.md (deflated 18%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/src/utils.hpp (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/src/docstrings.hpp (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/src/main.cpp (deflated 88%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/test_kompute.py (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/__init__.py (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/utils.py (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/test_logistic_regression.py (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/test_tensor_types.py (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/requirements-dev.txt (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/python/test/test_array_multiplication.py (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/GOVERNANCE.md (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/CHANGELOG.md (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.dockerignore (deflated 6%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/vcpkg.json.opt (deflated 23%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestMultipleAlgoExecutions.cpp (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/CMakeLists.txt (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestAsyncOperations.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestPushConstant.cpp (deflated 89%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestDestroy.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestLogisticRegression.cpp (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestOpTensorCreate.cpp (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestOpTensorSync.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestSequence.cpp (deflated 82%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestOpTensorCopy.cpp (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/CMakeLists.txt (deflated 21%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/Utils.cpp (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/glsl/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/glsl/CMakeLists.txt (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/glsl/test_op_custom_shader.comp (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/glsl/test_shader.comp (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/glsl/test_workgroup_shader.comp (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/glsl/test_logistic_regression_shader.comp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/shaders/Utils.hpp (deflated 38%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestSpecializationConstant.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestTensor.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestWorkgroup.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestManager.cpp (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/compiled_shaders_include/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/compiled_shaders_include/kompute_test/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/compiled_shaders_include/kompute_test/shaders/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/compiled_shaders_include/kompute_test/shaders/shadertest_workgroup.hpp (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/compiled_shaders_include/kompute_test/shaders/shadertest_logistic_regression.hpp (deflated 89%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/compiled_shaders_include/kompute_test/shaders/shadertest_op_custom_shader.hpp (deflated 82%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/test/TestOpShadersFromStringAndFile.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/SECURITY.md (deflated 39%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/.ccls (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/Dockerfile (deflated 24%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/scripts/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/scripts/requirements.txt (deflated 22%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/scripts/build_release_windows.sh (deflated 29%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/scripts/convert_shaders.py (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/scripts/build_release_linux.sh (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute/scripts/push_folder_to_branch.sh (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_mat_f32.comp (deflated 54%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q6_k.comp (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_k.comp (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_gelu.comp (deflated 42%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale_8.comp (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_1.comp (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_silu.comp (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_relu.comp (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/rope_common.comp (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_0.comp (deflated 55%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n_pre.comp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f32.comp (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f16.comp (deflated 49%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f16.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q8_0.comp (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_0.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/common.comp (deflated 70%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul.comp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q6_k.comp (deflated 57%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f16.comp (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f32.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_softmax.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale.comp (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_add.comp (deflated 59%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_f16.comp (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f16.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n.comp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f32.comp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_diagmask.comp (deflated 53%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rmsnorm.comp (deflated 62%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_addrow.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f32.comp (deflated 50%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_norm.comp (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f32.comp (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_1.comp (deflated 51%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f16.comp (deflated 68%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-kompute/ggml-kompute.cpp (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32_16.cl (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle_general.cl (deflated 89%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl.cl (deflated 89%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/embed_kernel.py (deflated 42%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_16.cl (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4.cl (deflated 76%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl (deflated 87%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32.cl (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle.cl (deflated 89%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/CMakeLists.txt (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opencl/ggml-opencl.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/CMakeLists.txt (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-threading.h (deflated 36%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-backend.cpp (deflated 83%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/llamafile/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.h (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.h (deflated 92%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp (deflated 88%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c (deflated 87%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.c (deflated 88%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/cpu-feats-x86.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.h (deflated 26%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/cmake/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/cmake/FindSIMD.cmake (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.h (deflated 27%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.h (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/amx/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/amx/common.h (deflated 61%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.h (deflated 56%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/amx/amx.h (deflated 28%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-impl.h (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/tsembd.hpp (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/common.hpp (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/dmmv.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/wkv6.cpp (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/norm.cpp (deflated 85%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/conv.hpp (deflated 41%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/gemm.hpp (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/conv.cpp (deflated 65%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/mmq.hpp (deflated 49%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/dpct/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/dpct/helper.hpp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/softmax.hpp (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/softmax.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/norm.hpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/CMakeLists.txt (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/dmmv.hpp (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/element_wise.cpp (deflated 92%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/convert.cpp (deflated 92%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/presets.hpp (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/mmvq.cpp (deflated 94%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/concat.cpp (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/convert.hpp (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/rope.hpp (deflated 43%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/ggml-sycl.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/mmvq.hpp (deflated 47%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/mmq.cpp (deflated 91%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/wkv6.hpp (deflated 36%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/concat.hpp (deflated 44%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/outprod.hpp (deflated 37%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/vecdotq.hpp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/tsembd.cpp (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/dequantize.hpp (deflated 84%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/common.cpp (deflated 64%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/im2col.hpp (deflated 45%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/outprod.cpp (deflated 63%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/element_wise.hpp (deflated 90%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/backend.hpp (deflated 54%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/rope.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-sycl/im2col.cpp (deflated 69%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-musa/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-musa/CMakeLists.txt (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-metal/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-metal/CMakeLists.txt (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-metal/ggml-metal-impl.h (deflated 87%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-metal/ggml-metal.m (deflated 88%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-metal/ggml-metal.metal (deflated 87%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-impl.h (deflated 73%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/ascendc_kernels.h (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/CMakeLists.txt (deflated 58%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/acl_tensor.cpp (deflated 71%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/Doxyfile (deflated 74%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/common.h (deflated 70%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/CMakeLists.txt (deflated 60%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/.clang-format (deflated 66%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/ggml-cann.cpp (deflated 81%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.h (deflated 77%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/acl_tensor.h (deflated 72%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-hip/ (stored 0%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-hip/CMakeLists.txt (deflated 67%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-backend-impl.h (deflated 78%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-threading.cpp (deflated 52%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-opt.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/ggml/src/ggml-backend-reg.cpp (deflated 78%)\n",
            "  adding: content/llama.cpp/LICENSE (deflated 41%)\n",
            "  adding: content/llama.cpp/poetry.lock (deflated 71%)\n",
            "  adding: content/llama.cpp/tests/ (stored 0%)\n",
            "  adding: content/llama.cpp/tests/test-sampling.cpp (deflated 82%)\n",
            "  adding: content/llama.cpp/tests/test-log.cpp (deflated 67%)\n",
            "  adding: content/llama.cpp/tests/test-tokenizer-random.py (deflated 73%)\n",
            "  adding: content/llama.cpp/tests/test-gguf.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/tests/test-rope.cpp (deflated 74%)\n",
            "  adding: content/llama.cpp/tests/test-opt.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/tests/test-lora-conversion-inference.sh (deflated 73%)\n",
            "  adding: content/llama.cpp/tests/test-tokenizer-1-spm.cpp (deflated 68%)\n",
            "  adding: content/llama.cpp/tests/test-backend-ops.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/tests/CMakeLists.txt (deflated 80%)\n",
            "  adding: content/llama.cpp/tests/test-tokenizer-1-bpe.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/tests/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/tests/test-llama-grammar.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/tests/test-quantize-perf.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/tests/test-grammar-parser.cpp (deflated 86%)\n",
            "  adding: content/llama.cpp/tests/test-model-load-cancel.cpp (deflated 51%)\n",
            "  adding: content/llama.cpp/tests/test-json-schema-to-grammar.cpp (deflated 88%)\n",
            "  adding: content/llama.cpp/tests/test-c.c (deflated 7%)\n",
            "  adding: content/llama.cpp/tests/test-quantize-fns.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/tests/test-chat-template.cpp (deflated 83%)\n",
            "  adding: content/llama.cpp/tests/test-tokenizer-0.sh (deflated 58%)\n",
            "  adding: content/llama.cpp/tests/test-grammar-integration.cpp (deflated 84%)\n",
            "  adding: content/llama.cpp/tests/test-double-float.cpp (deflated 53%)\n",
            "  adding: content/llama.cpp/tests/test-tokenizer-0.cpp (deflated 74%)\n",
            "  adding: content/llama.cpp/tests/test-autorelease.cpp (deflated 50%)\n",
            "  adding: content/llama.cpp/tests/get-model.h (stored 0%)\n",
            "  adding: content/llama.cpp/tests/test-tokenizer-0.py (deflated 68%)\n",
            "  adding: content/llama.cpp/tests/run-json-schema-to-grammar.mjs (deflated 39%)\n",
            "  adding: content/llama.cpp/tests/get-model.cpp (deflated 45%)\n",
            "  adding: content/llama.cpp/tests/test-barrier.cpp (deflated 65%)\n",
            "  adding: content/llama.cpp/tests/test-arg-parser.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/prompts/ (stored 0%)\n",
            "  adding: content/llama.cpp/prompts/chat.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/prompts/chat-with-baichuan.txt (deflated 3%)\n",
            "  adding: content/llama.cpp/prompts/alpaca.txt (deflated 18%)\n",
            "  adding: content/llama.cpp/prompts/parallel-questions.txt (deflated 50%)\n",
            "  adding: content/llama.cpp/prompts/chat-with-vicuna-v1.txt (deflated 43%)\n",
            "  adding: content/llama.cpp/prompts/reason-act.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/prompts/assistant.txt (deflated 60%)\n",
            "  adding: content/llama.cpp/prompts/dan-modified.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/prompts/mnemonics.txt (deflated 52%)\n",
            "  adding: content/llama.cpp/prompts/chat-with-vicuna-v0.txt (deflated 44%)\n",
            "  adding: content/llama.cpp/prompts/chat-with-qwen.txt (stored 0%)\n",
            "  adding: content/llama.cpp/prompts/dan.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/prompts/chat-with-bob.txt (deflated 38%)\n",
            "  adding: content/llama.cpp/prompts/LLM-questions.txt (deflated 76%)\n",
            "  adding: content/llama.cpp/.devops/ (stored 0%)\n",
            "  adding: content/llama.cpp/.devops/llama-cli-cann.Dockerfile (deflated 71%)\n",
            "  adding: content/llama.cpp/.devops/llama-cpp.srpm.spec (deflated 51%)\n",
            "  adding: content/llama.cpp/.devops/vulkan.Dockerfile (deflated 62%)\n",
            "  adding: content/llama.cpp/.devops/nix/ (stored 0%)\n",
            "  adding: content/llama.cpp/.devops/nix/nixpkgs-instances.nix (deflated 54%)\n",
            "  adding: content/llama.cpp/.devops/nix/docker.nix (deflated 46%)\n",
            "  adding: content/llama.cpp/.devops/nix/python-scripts.nix (deflated 51%)\n",
            "  adding: content/llama.cpp/.devops/nix/scope.nix (deflated 61%)\n",
            "  adding: content/llama.cpp/.devops/nix/package-gguf-py.nix (deflated 48%)\n",
            "  adding: content/llama.cpp/.devops/nix/apps.nix (deflated 51%)\n",
            "  adding: content/llama.cpp/.devops/nix/sif.nix (deflated 40%)\n",
            "  adding: content/llama.cpp/.devops/nix/package.nix (deflated 60%)\n",
            "  adding: content/llama.cpp/.devops/nix/devshells.nix (deflated 65%)\n",
            "  adding: content/llama.cpp/.devops/nix/jetson-support.nix (deflated 62%)\n",
            "  adding: content/llama.cpp/.devops/rocm.Dockerfile (deflated 59%)\n",
            "  adding: content/llama.cpp/.devops/intel.Dockerfile (deflated 62%)\n",
            "  adding: content/llama.cpp/.devops/cpu.Dockerfile (deflated 62%)\n",
            "  adding: content/llama.cpp/.devops/musa.Dockerfile (deflated 62%)\n",
            "  adding: content/llama.cpp/.devops/cuda.Dockerfile (deflated 60%)\n",
            "  adding: content/llama.cpp/.devops/cloud-v-pipeline (deflated 53%)\n",
            "  adding: content/llama.cpp/.devops/llama-cpp-cuda.srpm.spec (deflated 51%)\n",
            "  adding: content/llama.cpp/.devops/tools.sh (deflated 62%)\n",
            "  adding: content/llama.cpp/CONTRIBUTING.md (deflated 46%)\n",
            "  adding: content/llama.cpp/mypy.ini (deflated 37%)\n",
            "  adding: content/llama.cpp/AUTHORS (deflated 61%)\n",
            "  adding: content/llama.cpp/.github/ (stored 0%)\n",
            "  adding: content/llama.cpp/.github/labeler.yml (deflated 79%)\n",
            "  adding: content/llama.cpp/.github/workflows/ (stored 0%)\n",
            "  adding: content/llama.cpp/.github/workflows/gguf-publish.yml (deflated 47%)\n",
            "  adding: content/llama.cpp/.github/workflows/labeler.yml (deflated 41%)\n",
            "  adding: content/llama.cpp/.github/workflows/server.yml (deflated 71%)\n",
            "  adding: content/llama.cpp/.github/workflows/editorconfig.yml (deflated 49%)\n",
            "  adding: content/llama.cpp/.github/workflows/python-type-check.yml (deflated 58%)\n",
            "  adding: content/llama.cpp/.github/workflows/python-check-requirements.yml (deflated 61%)\n",
            "  adding: content/llama.cpp/.github/workflows/build.yml (deflated 84%)\n",
            "  adding: content/llama.cpp/.github/workflows/docker.yml (deflated 71%)\n",
            "  adding: content/llama.cpp/.github/workflows/bench.yml.disabled (deflated 71%)\n",
            "  adding: content/llama.cpp/.github/workflows/close-issue.yml (deflated 51%)\n",
            "  adding: content/llama.cpp/.github/workflows/python-lint.yml (deflated 50%)\n",
            "  adding: content/llama.cpp/.github/pull_request_template.md (deflated 11%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/ (stored 0%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/020-enhancement.yml (deflated 63%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/030-research.yml (deflated 59%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/040-refactor.yml (deflated 56%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/011-bug-results.yml (deflated 60%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/010-bug-compilation.yml (deflated 59%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/config.yml (deflated 53%)\n",
            "  adding: content/llama.cpp/.github/ISSUE_TEMPLATE/019-bug-misc.yml (deflated 59%)\n",
            "  adding: content/llama.cpp/examples/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/reason-act.sh (deflated 32%)\n",
            "  adding: content/llama.cpp/examples/gen-docs/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gen-docs/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/gen-docs/gen-docs.cpp (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/ts-type-to-grammar.sh (deflated 49%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist (deflated 21%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/contents.xcworkspacedata (deflated 27%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj (deflated 81%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/Contents.json (deflated 34%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/Contents.json (deflated 16%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/llama_swiftuiApp.swift (deflated 24%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/InputButton.swift (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/LoadCustomButton.swift (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/ContentView.swift (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/.gitignore (deflated 8%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/README.md (deflated 38%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/LibLlama.swift (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/Makefile (deflated 38%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/.gitignore (deflated 29%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/README.md (deflated 11%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/Package.swift (deflated 51%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/Sources/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/batched.swift/Sources/main.swift (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/gritlm/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gritlm/CMakeLists.txt (deflated 29%)\n",
            "  adding: content/llama.cpp/examples/gritlm/README.md (deflated 57%)\n",
            "  adding: content/llama.cpp/examples/gritlm/gritlm.cpp (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/server/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/chat-llama2.sh (deflated 54%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/readme.md (deflated 61%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/simplechat.js (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/ui.mjs (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/datautils.mjs (deflated 68%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/index.html (deflated 62%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/simplechat.css (deflated 59%)\n",
            "  adding: content/llama.cpp/examples/server/public_simplechat/simplechat_screens.webp (deflated 1%)\n",
            "  adding: content/llama.cpp/examples/server/themes/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/llamapattern.png (deflated 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/index.html (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/llama_cpp.png (deflated 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/favicon.ico (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/README.md (deflated 9%)\n",
            "  adding: content/llama.cpp/examples/server/themes/wild/wild.png (deflated 1%)\n",
            "  adding: content/llama.cpp/examples/server/themes/README.md (deflated 21%)\n",
            "  adding: content/llama.cpp/examples/server/themes/buttons-top/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/buttons-top/index.html (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/server/themes/buttons-top/favicon.ico (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/themes/buttons-top/README.md (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/server/themes/buttons-top/buttons_top.png (deflated 2%)\n",
            "  adding: content/llama.cpp/examples/server/CMakeLists.txt (deflated 50%)\n",
            "  adding: content/llama.cpp/examples/server/utils.hpp (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/server/server.cpp (deflated 81%)\n",
            "  adding: content/llama.cpp/examples/server/chat.sh (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/server/tests/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/tests/requirements.txt (deflated 14%)\n",
            "  adding: content/llama.cpp/examples/server/tests/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/tests/utils.py (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/server/tests/README.md (deflated 54%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_embedding.py (deflated 78%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_tokenize.py (deflated 69%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_slot_save.py (deflated 79%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_completion.py (deflated 81%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_security.py (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_ctx_shift.py (deflated 61%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_lora.py (deflated 51%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_rerank.py (deflated 62%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_basic.py (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_speculative.py (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_infill.py (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/server/tests/unit/test_chat_completion.py (deflated 78%)\n",
            "  adding: content/llama.cpp/examples/server/tests/conftest.py (deflated 39%)\n",
            "  adding: content/llama.cpp/examples/server/tests/tests.sh (deflated 21%)\n",
            "  adding: content/llama.cpp/examples/server/httplib.h (deflated 81%)\n",
            "  adding: content/llama.cpp/examples/server/chat.mjs (deflated 62%)\n",
            "  adding: content/llama.cpp/examples/server/bench/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/bench/requirements.txt (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/bench/prometheus.yml (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/server/bench/bench.py (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/server/bench/script.js (deflated 68%)\n",
            "  adding: content/llama.cpp/examples/server/bench/README.md (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/server/README.md (deflated 68%)\n",
            "  adding: content/llama.cpp/examples/server/webui/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/webui/package.json (deflated 49%)\n",
            "  adding: content/llama.cpp/examples/server/webui/postcss.config.js (deflated 18%)\n",
            "  adding: content/llama.cpp/examples/server/webui/index.html (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/server/webui/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/webui/src/main.js (deflated 69%)\n",
            "  adding: content/llama.cpp/examples/server/webui/src/highlight-config.js (deflated 79%)\n",
            "  adding: content/llama.cpp/examples/server/webui/src/styles.scss (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/server/webui/src/katex-gpt.js (deflated 58%)\n",
            "  adding: content/llama.cpp/examples/server/webui/public/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/webui/public/demo-conversation.json (deflated 56%)\n",
            "  adding: content/llama.cpp/examples/server/webui/vite.config.js (deflated 51%)\n",
            "  adding: content/llama.cpp/examples/server/webui/tailwind.config.js (deflated 42%)\n",
            "  adding: content/llama.cpp/examples/server/webui/package-lock.json (deflated 78%)\n",
            "  adding: content/llama.cpp/examples/server/public/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/public/index.html.gz (deflated 0%)\n",
            "  adding: content/llama.cpp/examples/server/public/loading.html (deflated 39%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/colorthemes.css (deflated 82%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/theme-snowstorm.css (deflated 84%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/index.js (deflated 62%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/completion.js (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/system-prompts.js (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/index.html (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/favicon.ico (stored 0%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/theme-polarnight.css (deflated 84%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/prompt-formats.js (deflated 83%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/theme-mangotango.css (deflated 82%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/loading.html (deflated 39%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/theme-playground.css (deflated 83%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/theme-ketivah.css (deflated 84%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/index-new.html (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/theme-beeninorder.css (deflated 83%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/style.css (deflated 82%)\n",
            "  adding: content/llama.cpp/examples/server/public_legacy/json-schema-to-grammar.mjs (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/deprecation-warning/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/deprecation-warning/README.md (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/deprecation-warning/deprecation-warning.cpp (deflated 53%)\n",
            "  adding: content/llama.cpp/examples/batched-bench/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/batched-bench/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/batched-bench/batched-bench.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/batched-bench/README.md (deflated 62%)\n",
            "  adding: content/llama.cpp/examples/CMakeLists.txt (deflated 68%)\n",
            "  adding: content/llama.cpp/examples/infill/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/infill/infill.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/infill/CMakeLists.txt (deflated 29%)\n",
            "  adding: content/llama.cpp/examples/infill/README.md (deflated 56%)\n",
            "  adding: content/llama.cpp/examples/main-cmake-pkg/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/main-cmake-pkg/CMakeLists.txt (deflated 49%)\n",
            "  adding: content/llama.cpp/examples/main-cmake-pkg/.gitignore (deflated 41%)\n",
            "  adding: content/llama.cpp/examples/main-cmake-pkg/README.md (deflated 52%)\n",
            "  adding: content/llama.cpp/examples/passkey/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/passkey/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/passkey/README.md (deflated 38%)\n",
            "  adding: content/llama.cpp/examples/passkey/passkey.cpp (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/gbnf-validator/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gbnf-validator/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/gbnf-validator/gbnf-validator.cpp (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/jeopardy/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/jeopardy/jeopardy.sh (deflated 44%)\n",
            "  adding: content/llama.cpp/examples/jeopardy/questions.txt (deflated 50%)\n",
            "  adding: content/llama.cpp/examples/jeopardy/README.md (deflated 45%)\n",
            "  adding: content/llama.cpp/examples/jeopardy/qasheet.csv (deflated 51%)\n",
            "  adding: content/llama.cpp/examples/jeopardy/graph.py (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/gguf/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf/CMakeLists.txt (deflated 27%)\n",
            "  adding: content/llama.cpp/examples/gguf/gguf.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/convert-llama2c-to-ggml/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/examples/convert-llama2c-to-ggml/CMakeLists.txt (deflated 33%)\n",
            "  adding: content/llama.cpp/examples/convert-llama2c-to-ggml/README.md (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/rpc/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/rpc/rpc-server.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/rpc/CMakeLists.txt (deflated 19%)\n",
            "  adding: content/llama.cpp/examples/rpc/README.md (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/parallel/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/parallel/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/parallel/README.md (deflated 15%)\n",
            "  adding: content/llama.cpp/examples/parallel/parallel.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/imatrix/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/imatrix/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/imatrix/README.md (deflated 53%)\n",
            "  adding: content/llama.cpp/examples/imatrix/imatrix.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/lookahead/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/lookahead/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/lookahead/README.md (deflated 28%)\n",
            "  adding: content/llama.cpp/examples/lookahead/lookahead.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/chat.sh (deflated 34%)\n",
            "  adding: content/llama.cpp/examples/main/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/main/CMakeLists.txt (deflated 28%)\n",
            "  adding: content/llama.cpp/examples/main/README.md (deflated 66%)\n",
            "  adding: content/llama.cpp/examples/main/main.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/negative.txt (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/mean.hpp (deflated 63%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/positive.txt (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/CMakeLists.txt (deflated 32%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/cvector-generator.cpp (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/README.md (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/completions.txt (deflated 65%)\n",
            "  adding: content/llama.cpp/examples/cvector-generator/pca.hpp (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/llama.vim (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/sycl/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/sycl/ls-sycl-device.cpp (deflated 19%)\n",
            "  adding: content/llama.cpp/examples/sycl/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/sycl/build.sh (deflated 49%)\n",
            "  adding: content/llama.cpp/examples/sycl/win-build-sycl.bat (deflated 50%)\n",
            "  adding: content/llama.cpp/examples/sycl/README.md (deflated 59%)\n",
            "  adding: content/llama.cpp/examples/sycl/run-llama2.sh (deflated 43%)\n",
            "  adding: content/llama.cpp/examples/sycl/win-run-llama2.bat (deflated 22%)\n",
            "  adding: content/llama.cpp/examples/perplexity/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/perplexity/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/perplexity/README.md (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/perplexity/perplexity.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/examples/llama.android/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/gradle.properties (deflated 48%)\n",
            "  adding: content/llama.cpp/examples/llama.android/.gitignore (deflated 34%)\n",
            "  adding: content/llama.cpp/examples/llama.android/gradlew (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/Downloadable.kt (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Type.kt (deflated 68%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Theme.kt (deflated 65%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Color.kt (deflated 45%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainActivity.kt (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_background.xml (deflated 93%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_foreground.xml (deflated 63%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher_round.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher_round.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/xml/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/xml/data_extraction_rules.xml (deflated 49%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/xml/backup_rules.xml (deflated 43%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/values/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/values/strings.xml (deflated 20%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/values/colors.xml (deflated 58%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/values/themes.xml (deflated 17%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher_round.xml (deflated 53%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher.xml (deflated 53%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher_round.webp (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/src/main/AndroidManifest.xml (deflated 61%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/build.gradle.kts (deflated 66%)\n",
            "  adding: content/llama.cpp/examples/llama.android/app/proguard-rules.pro (deflated 44%)\n",
            "  adding: content/llama.cpp/examples/llama.android/README.md (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/.gitignore (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/androidTest/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt (deflated 49%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/java/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/java/android/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/AndroidManifest.xml (deflated 17%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/cpp/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/cpp/CMakeLists.txt (deflated 53%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/main/cpp/llama-android.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/test/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/test/java/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/test/java/android/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt (deflated 33%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/build.gradle.kts (deflated 60%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/proguard-rules.pro (deflated 44%)\n",
            "  adding: content/llama.cpp/examples/llama.android/llama/consumer-rules.pro (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/gradle/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/gradle/wrapper/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.jar (deflated 10%)\n",
            "  adding: content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.properties (deflated 35%)\n",
            "  adding: content/llama.cpp/examples/llama.android/build.gradle.kts (deflated 42%)\n",
            "  adding: content/llama.cpp/examples/llama.android/settings.gradle.kts (deflated 48%)\n",
            "  adding: content/llama.cpp/examples/json_schema_to_grammar.py (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/llm.vim (deflated 45%)\n",
            "  adding: content/llama.cpp/examples/simple/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/simple/CMakeLists.txt (deflated 29%)\n",
            "  adding: content/llama.cpp/examples/simple/README.md (deflated 51%)\n",
            "  adding: content/llama.cpp/examples/simple/simple.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/lookup/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/lookup/CMakeLists.txt (deflated 79%)\n",
            "  adding: content/llama.cpp/examples/lookup/lookup-stats.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/lookup/README.md (deflated 43%)\n",
            "  adding: content/llama.cpp/examples/lookup/lookup-merge.cpp (deflated 64%)\n",
            "  adding: content/llama.cpp/examples/lookup/lookup-create.cpp (deflated 56%)\n",
            "  adding: content/llama.cpp/examples/lookup/lookup.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/chat-13B.bat (deflated 52%)\n",
            "  adding: content/llama.cpp/examples/convert_legacy_llama.py (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/regex_to_grammar.py (deflated 44%)\n",
            "  adding: content/llama.cpp/examples/llama-bench/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llama-bench/llama-bench.cpp (deflated 82%)\n",
            "  adding: content/llama.cpp/examples/llama-bench/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/llama-bench/README.md (deflated 77%)\n",
            "  adding: content/llama.cpp/examples/retrieval/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/retrieval/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/retrieval/README.md (deflated 50%)\n",
            "  adding: content/llama.cpp/examples/retrieval/retrieval.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/gguf-split/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-split/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/gguf-split/README.md (deflated 41%)\n",
            "  adding: content/llama.cpp/examples/gguf-split/gguf-split.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/examples/gguf-split/tests.sh (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/quantize/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/quantize/CMakeLists.txt (deflated 35%)\n",
            "  adding: content/llama.cpp/examples/quantize/quantize.cpp (deflated 77%)\n",
            "  adding: content/llama.cpp/examples/quantize/README.md (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/quantize/tests.sh (deflated 62%)\n",
            "  adding: content/llama.cpp/examples/chat-persistent.sh (deflated 64%)\n",
            "  adding: content/llama.cpp/examples/pydantic_models_to_grammar.py (deflated 82%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/CMakeLists.txt (deflated 54%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/README.md (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/gguf-hash.cpp (deflated 81%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/rotate-bits/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/rotate-bits/package.json (deflated 38%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/rotate-bits/rotate-bits.h (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha256/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.c (deflated 63%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha256/package.json (deflated 41%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.h (deflated 47%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/xxhash/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/xxhash/clib.json (deflated 33%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.h (deflated 77%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.c (deflated 48%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha1/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha1/package.json (deflated 35%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.h (deflated 54%)\n",
            "  adding: content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c (deflated 66%)\n",
            "  adding: content/llama.cpp/examples/server_embd.py (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/export-lora/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/export-lora/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/export-lora/README.md (deflated 59%)\n",
            "  adding: content/llama.cpp/examples/export-lora/export-lora.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/batched/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/batched/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/batched/README.md (deflated 55%)\n",
            "  adding: content/llama.cpp/examples/batched/batched.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/simple-chat/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/simple-chat/simple-chat.cpp (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/simple-chat/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/simple-chat/README.md (deflated 32%)\n",
            "  adding: content/llama.cpp/examples/chat-13B.sh (deflated 45%)\n",
            "  adding: content/llama.cpp/examples/embedding/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/embedding/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/embedding/README.md (deflated 63%)\n",
            "  adding: content/llama.cpp/examples/embedding/embedding.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/tts/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/tts/CMakeLists.txt (deflated 28%)\n",
            "  adding: content/llama.cpp/examples/tts/tts.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/tts/convert_pt_to_hf.py (deflated 63%)\n",
            "  adding: content/llama.cpp/examples/tts/tts-outetts.py (deflated 69%)\n",
            "  adding: content/llama.cpp/examples/chat-vicuna.sh (deflated 44%)\n",
            "  adding: content/llama.cpp/examples/pydantic_models_to_grammar_examples.py (deflated 71%)\n",
            "  adding: content/llama.cpp/examples/llava/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llava/convert_image_encoder_to_gguf.py (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/llava/llava_surgery.py (deflated 53%)\n",
            "  adding: content/llama.cpp/examples/llava/requirements.txt (deflated 21%)\n",
            "  adding: content/llama.cpp/examples/llava/minicpmv-cli.cpp (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/llava/CMakeLists.txt (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/llava/qwen2vl-cli.cpp (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/llava/llava-cli.cpp (deflated 74%)\n",
            "  adding: content/llama.cpp/examples/llava/README-minicpmv2.5.md (deflated 65%)\n",
            "  adding: content/llama.cpp/examples/llava/llava_surgery_v2.py (deflated 72%)\n",
            "  adding: content/llama.cpp/examples/llava/llava.h (deflated 64%)\n",
            "  adding: content/llama.cpp/examples/llava/MobileVLM-README.md (deflated 76%)\n",
            "  adding: content/llama.cpp/examples/llava/clip.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/examples/llava/README.md (deflated 61%)\n",
            "  adding: content/llama.cpp/examples/llava/minicpmv-surgery.py (deflated 59%)\n",
            "  adding: content/llama.cpp/examples/llava/README-minicpmv2.6.md (deflated 64%)\n",
            "  adding: content/llama.cpp/examples/llava/llava.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/llava/minicpmv-convert-image-encoder-to-gguf.py (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/llava/qwen2_vl_surgery.py (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/llava/clip.h (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/llava/android/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/llava/android/build_64.sh (deflated 24%)\n",
            "  adding: content/llama.cpp/examples/llava/android/adb_run.sh (deflated 73%)\n",
            "  adding: content/llama.cpp/examples/json_schema_pydantic_example.py (deflated 58%)\n",
            "  adding: content/llama.cpp/examples/eval-callback/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/eval-callback/CMakeLists.txt (deflated 42%)\n",
            "  adding: content/llama.cpp/examples/eval-callback/README.md (deflated 82%)\n",
            "  adding: content/llama.cpp/examples/eval-callback/eval-callback.cpp (deflated 67%)\n",
            "  adding: content/llama.cpp/examples/speculative/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/speculative/CMakeLists.txt (deflated 31%)\n",
            "  adding: content/llama.cpp/examples/speculative/README.md (deflated 50%)\n",
            "  adding: content/llama.cpp/examples/speculative/speculative.cpp (deflated 78%)\n",
            "  adding: content/llama.cpp/examples/save-load-state/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/save-load-state/CMakeLists.txt (deflated 32%)\n",
            "  adding: content/llama.cpp/examples/save-load-state/save-load-state.cpp (deflated 80%)\n",
            "  adding: content/llama.cpp/examples/server-llama2-13B.sh (deflated 38%)\n",
            "  adding: content/llama.cpp/examples/quantize-stats/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/quantize-stats/quantize-stats.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/quantize-stats/CMakeLists.txt (deflated 36%)\n",
            "  adding: content/llama.cpp/examples/speculative-simple/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/speculative-simple/CMakeLists.txt (deflated 33%)\n",
            "  adding: content/llama.cpp/examples/speculative-simple/README.md (deflated 45%)\n",
            "  adding: content/llama.cpp/examples/speculative-simple/speculative-simple.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/examples/run/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/run/CMakeLists.txt (deflated 28%)\n",
            "  adding: content/llama.cpp/examples/run/README.md (deflated 54%)\n",
            "  adding: content/llama.cpp/examples/run/run.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/examples/Miku.sh (deflated 54%)\n",
            "  adding: content/llama.cpp/examples/tokenize/ (stored 0%)\n",
            "  adding: content/llama.cpp/examples/tokenize/CMakeLists.txt (deflated 30%)\n",
            "  adding: content/llama.cpp/examples/tokenize/tokenize.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/README.md (deflated 65%)\n",
            "  adding: content/llama.cpp/.git/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/info/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/info/exclude (deflated 28%)\n",
            "  adding: content/llama.cpp/.git/index (deflated 62%)\n",
            "  adding: content/llama.cpp/.git/description (deflated 14%)\n",
            "  adding: content/llama.cpp/.git/HEAD (stored 0%)\n",
            "  adding: content/llama.cpp/.git/hooks/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/llama.cpp/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/llama.cpp/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/llama.cpp/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/llama.cpp/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/llama.cpp/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/llama.cpp/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/llama.cpp/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/llama.cpp/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/llama.cpp/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/llama.cpp/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/llama.cpp/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/llama.cpp/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/llama.cpp/.git/modules/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/info/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/info/exclude (deflated 28%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/index (deflated 60%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/description (deflated 14%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/HEAD (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/update.sample (deflated 68%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/tags/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/remotes/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/heads/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/refs/heads/master (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/branches/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/config (deflated 36%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/HEAD (deflated 52%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/refs/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/refs/remotes/origin/HEAD (deflated 30%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/logs/refs/heads/master (deflated 30%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/objects/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/objects/info/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/objects/pack/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/objects/pack/pack-f707d354d8fb041fb0a6c48f3e7c417de3ea78b5.idx (deflated 2%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/objects/pack/pack-f707d354d8fb041fb0a6c48f3e7c417de3ea78b5.pack (deflated 0%)\n",
            "  adding: content/llama.cpp/.git/modules/kompute/packed-refs (deflated 38%)\n",
            "  adding: content/llama.cpp/.git/refs/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/llama.cpp/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/llama.cpp/.git/branches/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/config (deflated 40%)\n",
            "  adding: content/llama.cpp/.git/logs/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/logs/HEAD (deflated 29%)\n",
            "  adding: content/llama.cpp/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/logs/refs/remotes/origin/HEAD (deflated 29%)\n",
            "  adding: content/llama.cpp/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/logs/refs/heads/master (deflated 29%)\n",
            "  adding: content/llama.cpp/.git/objects/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/objects/info/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/llama.cpp/.git/objects/pack/pack-635ea9d1470c0cf29d606761ecf43a65253faa22.idx (deflated 1%)\n",
            "  adding: content/llama.cpp/.git/objects/pack/pack-635ea9d1470c0cf29d606761ecf43a65253faa22.pack (deflated 1%)\n",
            "  adding: content/llama.cpp/.git/packed-refs (deflated 54%)\n",
            "  adding: content/llama.cpp/.editorconfig (deflated 61%)\n",
            "  adding: content/llama.cpp/convert_lora_to_gguf.py (deflated 72%)\n",
            "  adding: content/llama.cpp/llama-cli (deflated 62%)\n",
            "  adding: content/llama.cpp/.clang-format (deflated 62%)\n",
            "  adding: content/llama.cpp/pyrightconfig.json (deflated 47%)\n",
            "  adding: content/llama.cpp/.clang-tidy (deflated 54%)\n",
            "  adding: content/llama.cpp/common/ (stored 0%)\n",
            "  adding: content/llama.cpp/common/common.h (deflated 71%)\n",
            "  adding: content/llama.cpp/common/json-schema-to-grammar.h (deflated 27%)\n",
            "  adding: content/llama.cpp/common/arg.h (deflated 75%)\n",
            "  adding: content/llama.cpp/common/CMakeLists.txt (deflated 62%)\n",
            "  adding: content/llama.cpp/common/arg.cpp (deflated 82%)\n",
            "  adding: content/llama.cpp/common/build-info.cpp (deflated 32%)\n",
            "  adding: content/llama.cpp/common/ngram-cache.h (deflated 66%)\n",
            "  adding: content/llama.cpp/common/speculative.h (deflated 62%)\n",
            "  adding: content/llama.cpp/common/stb_image.h (deflated 75%)\n",
            "  adding: content/llama.cpp/common/ngram-cache.cpp (deflated 78%)\n",
            "  adding: content/llama.cpp/common/console.h (deflated 44%)\n",
            "  adding: content/llama.cpp/common/build-info.cpp.in (deflated 53%)\n",
            "  adding: content/llama.cpp/common/json-schema-to-grammar.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/common/sampling.h (deflated 68%)\n",
            "  adding: content/llama.cpp/common/sampling.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/common/log.h (deflated 70%)\n",
            "  adding: content/llama.cpp/common/cmake/ (stored 0%)\n",
            "  adding: content/llama.cpp/common/cmake/build-info-gen-cpp.cmake (deflated 63%)\n",
            "  adding: content/llama.cpp/common/common.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/common/json.hpp (deflated 85%)\n",
            "  adding: content/llama.cpp/common/base64.hpp (deflated 76%)\n",
            "  adding: content/llama.cpp/common/log.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/common/speculative.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/common/console.cpp (deflated 75%)\n",
            "  adding: content/llama.cpp/gguf-py/ (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/pyproject.toml (deflated 50%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/ (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/py.typed (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/quants.py (deflated 75%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/gguf_reader.py (deflated 71%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__init__.py (deflated 57%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/gguf_writer.py (deflated 79%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/metadata.py (deflated 81%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/vocab.py (deflated 76%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/tensor_mapping.py (deflated 87%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/lazy.py (deflated 69%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/constants.py (deflated 86%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/gguf.py (deflated 42%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/utility.py (deflated 70%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/ (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/gguf_writer.cpython-310.pyc (deflated 66%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/__init__.cpython-310.pyc (deflated 33%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/tensor_mapping.cpython-310.pyc (deflated 72%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/lazy.cpython-310.pyc (deflated 51%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/utility.cpython-310.pyc (deflated 42%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/gguf_reader.cpython-310.pyc (deflated 48%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/constants.cpython-310.pyc (deflated 65%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/quants.cpython-310.pyc (deflated 61%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/metadata.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/llama.cpp/gguf-py/gguf/__pycache__/vocab.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/llama.cpp/gguf-py/LICENSE (deflated 41%)\n",
            "  adding: content/llama.cpp/gguf-py/tests/ (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/tests/test_metadata.py (deflated 78%)\n",
            "  adding: content/llama.cpp/gguf-py/tests/test_quants.py (deflated 73%)\n",
            "  adding: content/llama.cpp/gguf-py/tests/__init__.py (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/examples/ (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/examples/reader.py (deflated 57%)\n",
            "  adding: content/llama.cpp/gguf-py/examples/writer.py (deflated 57%)\n",
            "  adding: content/llama.cpp/gguf-py/README.md (deflated 59%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/ (stored 0%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/gguf_hash.py (deflated 63%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/__init__.py (deflated 60%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/gguf_new_metadata.py (deflated 71%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/gguf_convert_endian.py (deflated 63%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/gguf_dump.py (deflated 75%)\n",
            "  adding: content/llama.cpp/gguf-py/scripts/gguf_set_metadata.py (deflated 60%)\n",
            "  adding: content/llama.cpp/src/ (stored 0%)\n",
            "  adding: content/llama.cpp/src/unicode-data.h (deflated 60%)\n",
            "  adding: content/llama.cpp/src/CMakeLists.txt (deflated 54%)\n",
            "  adding: content/llama.cpp/src/llama-grammar.h (deflated 70%)\n",
            "  adding: content/llama.cpp/src/llama-sampling.h (deflated 74%)\n",
            "  adding: content/llama.cpp/src/llama-impl.h (deflated 72%)\n",
            "  adding: content/llama.cpp/src/llama-sampling.cpp (deflated 81%)\n",
            "  adding: content/llama.cpp/src/unicode-data.cpp (deflated 78%)\n",
            "  adding: content/llama.cpp/src/unicode.cpp (deflated 81%)\n",
            "  adding: content/llama.cpp/src/llama-vocab.cpp (deflated 79%)\n",
            "  adding: content/llama.cpp/src/llama-grammar.cpp (deflated 81%)\n",
            "  adding: content/llama.cpp/src/llama-vocab.h (deflated 76%)\n",
            "  adding: content/llama.cpp/src/llama.cpp (deflated 87%)\n",
            "  adding: content/llama.cpp/src/unicode.h (deflated 68%)\n",
            "  adding: content/llama.cpp/cmake/ (stored 0%)\n",
            "  adding: content/llama.cpp/cmake/llama.pc.in (deflated 27%)\n",
            "  adding: content/llama.cpp/cmake/arm64-apple-clang.cmake (deflated 51%)\n",
            "  adding: content/llama.cpp/cmake/llama-config.cmake.in (deflated 72%)\n",
            "  adding: content/llama.cpp/cmake/arm64-windows-llvm.cmake (deflated 51%)\n",
            "  adding: content/llama.cpp/cmake/arm64-windows-msvc.cmake (deflated 42%)\n",
            "  adding: content/llama.cpp/cmake/build-info.cmake (deflated 65%)\n",
            "  adding: content/llama.cpp/cmake/common.cmake (deflated 64%)\n",
            "  adding: content/llama.cpp/cmake/git-vars.cmake (deflated 61%)\n",
            "  adding: content/llama.cpp/cmake/x64-windows-llvm.cmake (deflated 47%)\n",
            "  adding: content/llama.cpp/convert_hf_to_gguf_update.py (deflated 72%)\n",
            "  adding: content/llama.cpp/docs/ (stored 0%)\n",
            "  adding: content/llama.cpp/docs/install.md (deflated 53%)\n",
            "  adding: content/llama.cpp/docs/build.md (deflated 65%)\n",
            "  adding: content/llama.cpp/docs/docker.md (deflated 76%)\n",
            "  adding: content/llama.cpp/docs/backend/ (stored 0%)\n",
            "  adding: content/llama.cpp/docs/backend/SYCL.md (deflated 70%)\n",
            "  adding: content/llama.cpp/docs/backend/CANN.md (deflated 68%)\n",
            "  adding: content/llama.cpp/docs/backend/BLIS.md (deflated 44%)\n",
            "  adding: content/llama.cpp/docs/development/ (stored 0%)\n",
            "  adding: content/llama.cpp/docs/development/HOWTO-add-model.md (deflated 60%)\n",
            "  adding: content/llama.cpp/docs/development/llama-star/ (stored 0%)\n",
            "  adding: content/llama.cpp/docs/development/llama-star/idea-arch.key (deflated 24%)\n",
            "  adding: content/llama.cpp/docs/development/llama-star/idea-arch.pdf (deflated 27%)\n",
            "  adding: content/llama.cpp/docs/development/debugging-tests.md (deflated 56%)\n",
            "  adding: content/llama.cpp/docs/development/token_generation_performance_tips.md (deflated 51%)\n",
            "  adding: content/llama.cpp/docs/android.md (deflated 51%)\n",
            "  adding: content/llama.cpp/llama-quantize (deflated 61%)\n",
            "  adding: content/llama.cpp/Package.swift (deflated 49%)\n",
            "  adding: content/llama.cpp/spm-headers/ (stored 0%)\n",
            "  adding: content/llama.cpp/spm-headers/ggml-cpp.h (deflated 74%)\n",
            "  adding: content/llama.cpp/spm-headers/ggml-backend.h (deflated 80%)\n",
            "  adding: content/llama.cpp/spm-headers/ggml-cpu.h (deflated 77%)\n",
            "  adding: content/llama.cpp/spm-headers/ggml-alloc.h (deflated 68%)\n",
            "  adding: content/llama.cpp/spm-headers/llama.h (deflated 77%)\n",
            "  adding: content/llama.cpp/spm-headers/ggml.h (deflated 83%)\n",
            "  adding: content/llama.cpp/spm-headers/ggml-metal.h (deflated 58%)\n",
            "  adding: content/llama.cpp/flake.lock (deflated 66%)\n",
            "  adding: content/llama.cpp/requirements/ (stored 0%)\n",
            "  adding: content/llama.cpp/requirements/requirements-test-tokenizer-random.txt (stored 0%)\n",
            "  adding: content/llama.cpp/requirements/requirements-convert_legacy_llama.txt (deflated 13%)\n",
            "  adding: content/llama.cpp/requirements/requirements-compare-llama-bench.txt (stored 0%)\n",
            "  adding: content/llama.cpp/requirements/requirements-convert_lora_to_gguf.txt (deflated 13%)\n",
            "  adding: content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (deflated 10%)\n",
            "  adding: content/llama.cpp/requirements/requirements-pydantic.txt (stored 0%)\n",
            "  adding: content/llama.cpp/requirements/requirements-convert_hf_to_gguf_update.txt (deflated 10%)\n",
            "  adding: content/llama.cpp/requirements/requirements-all.txt (deflated 66%)\n",
            "  adding: content/llama.cpp/requirements/requirements-convert_llama_ggml_to_gguf.txt (stored 0%)\n",
            "  adding: content/llama.cpp/.ecrc (deflated 9%)\n",
            "  adding: content/llama.cpp/.dockerignore (deflated 27%)\n",
            "  adding: content/llama.cpp/Sources/ (stored 0%)\n",
            "  adding: content/llama.cpp/Sources/llama/ (stored 0%)\n",
            "  adding: content/llama.cpp/Sources/llama/llama.h (stored 0%)\n",
            "  adding: content/llama.cpp/Sources/llama/module.modulemap (deflated 19%)\n",
            "  adding: content/llama.cpp/pocs/ (stored 0%)\n",
            "  adding: content/llama.cpp/pocs/CMakeLists.txt (deflated 21%)\n",
            "  adding: content/llama.cpp/pocs/vdot/ (stored 0%)\n",
            "  adding: content/llama.cpp/pocs/vdot/CMakeLists.txt (deflated 59%)\n",
            "  adding: content/llama.cpp/pocs/vdot/vdot.cpp (deflated 73%)\n",
            "  adding: content/llama.cpp/pocs/vdot/q8dot.cpp (deflated 70%)\n",
            "  adding: content/llama.cpp/llama-export-lora (deflated 62%)\n",
            "  adding: content/llama.cpp/SECURITY.md (deflated 55%)\n",
            "  adding: content/llama.cpp/scripts/ (stored 0%)\n",
            "  adding: content/llama.cpp/scripts/check-requirements.sh (deflated 57%)\n",
            "  adding: content/llama.cpp/scripts/install-oneapi.bat (deflated 56%)\n",
            "  adding: content/llama.cpp/scripts/gen-authors.sh (deflated 30%)\n",
            "  adding: content/llama.cpp/scripts/ci-run.sh (deflated 51%)\n",
            "  adding: content/llama.cpp/scripts/get-wikitext-2.sh (deflated 34%)\n",
            "  adding: content/llama.cpp/scripts/qnt-all.sh (deflated 44%)\n",
            "  adding: content/llama.cpp/scripts/verify-checksum-models.py (deflated 59%)\n",
            "  adding: content/llama.cpp/scripts/hf.sh (deflated 61%)\n",
            "  adding: content/llama.cpp/scripts/get-pg.sh (deflated 50%)\n",
            "  adding: content/llama.cpp/scripts/get-winogrande.sh (deflated 38%)\n",
            "  adding: content/llama.cpp/scripts/run-all-ppl.sh (deflated 40%)\n",
            "  adding: content/llama.cpp/scripts/get-wikitext-103.sh (deflated 27%)\n",
            "  adding: content/llama.cpp/scripts/get-flags.mk (deflated 61%)\n",
            "  adding: content/llama.cpp/scripts/xxd.cmake (deflated 43%)\n",
            "  adding: content/llama.cpp/scripts/get-hellaswag.sh (deflated 36%)\n",
            "  adding: content/llama.cpp/scripts/sync-ggml-am.sh (deflated 75%)\n",
            "  adding: content/llama.cpp/scripts/sync-ggml.sh (deflated 79%)\n",
            "  adding: content/llama.cpp/scripts/build-info.sh (deflated 53%)\n",
            "  adding: content/llama.cpp/scripts/compare-llama-bench.py (deflated 69%)\n",
            "  adding: content/llama.cpp/scripts/compare-commits.sh (deflated 51%)\n",
            "  adding: content/llama.cpp/scripts/gen-unicode-data.py (deflated 69%)\n",
            "  adding: content/llama.cpp/scripts/debug-test.sh (deflated 60%)\n",
            "  adding: content/llama.cpp/scripts/run-all-perf.sh (deflated 41%)\n",
            "  adding: content/llama.cpp/scripts/sync-ggml.last (stored 0%)\n",
            "  adding: content/llama.cpp/grammars/ (stored 0%)\n",
            "  adding: content/llama.cpp/grammars/json_arr.gbnf (deflated 46%)\n",
            "  adding: content/llama.cpp/grammars/japanese.gbnf (deflated 33%)\n",
            "  adding: content/llama.cpp/grammars/chess.gbnf (deflated 44%)\n",
            "  adding: content/llama.cpp/grammars/README.md (deflated 67%)\n",
            "  adding: content/llama.cpp/grammars/arithmetic.gbnf (deflated 33%)\n",
            "  adding: content/llama.cpp/grammars/json.gbnf (deflated 44%)\n",
            "  adding: content/llama.cpp/grammars/english.gbnf (deflated 31%)\n",
            "  adding: content/llama.cpp/grammars/c.gbnf (deflated 64%)\n",
            "  adding: content/llama.cpp/grammars/list.gbnf (deflated 8%)\n",
            "  adding: content/lora_model/ (stored 0%)\n",
            "  adding: content/lora_model/adapter_config.json (deflated 56%)\n",
            "  adding: content/lora_model/tokenizer.json (deflated 85%)\n",
            "  adding: content/lora_model/README.md (deflated 66%)\n",
            "  adding: content/lora_model/tokenizer_config.json (deflated 94%)\n",
            "  adding: content/lora_model/special_tokens_map.json (deflated 71%)\n",
            "  adding: content/lora_model/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/unsloth.Q5_K_M.gguf (deflated 1%)\n",
            "  adding: content/model/unsloth.F16.gguf (deflated 10%)\n",
            "  adding: content/model/tokenizer.json (deflated 85%)\n",
            "  adding: content/model/generation_config.json (deflated 38%)\n",
            "  adding: content/model/pytorch_model.bin.index.json (deflated 96%)\n",
            "  adding: content/model/tokenizer_config.json (deflated 94%)\n",
            "  adding: content/model/pytorch_model-00002-of-00002.bin (deflated 8%)\n",
            "  adding: content/model/pytorch_model-00001-of-00002.bin (deflated 11%)\n",
            "  adding: content/model/unsloth.Q4_K_M.gguf (deflated 2%)\n",
            "  adding: content/model/special_tokens_map.json (deflated 71%)\n",
            "  adding: content/model/config.json (deflated 52%)\n",
            "  adding: content/model/unsloth.Q8_0.gguf (deflated 23%)\n",
            "  adding: content/outputs/ (stored 0%)\n",
            "  adding: content/outputs/checkpoint-60/ (stored 0%)\n",
            "  adding: content/outputs/checkpoint-60/adapter_config.json (deflated 56%)\n",
            "  adding: content/outputs/checkpoint-60/tokenizer.json (deflated 85%)\n",
            "  adding: content/outputs/checkpoint-60/scheduler.pt (deflated 56%)\n",
            "  adding: content/outputs/checkpoint-60/trainer_state.json (deflated 80%)\n",
            "  adding: content/outputs/checkpoint-60/optimizer.pt (deflated 10%)\n",
            "  adding: content/outputs/checkpoint-60/README.md (deflated 66%)\n",
            "  adding: content/outputs/checkpoint-60/tokenizer_config.json (deflated 94%)\n",
            "  adding: content/outputs/checkpoint-60/training_args.bin (deflated 51%)\n",
            "  adding: content/outputs/checkpoint-60/rng_state.pth (deflated 25%)\n",
            "  adding: content/outputs/checkpoint-60/special_tokens_map.json (deflated 71%)\n",
            "  adding: content/outputs/checkpoint-60/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/outputs/runs/ (stored 0%)\n",
            "  adding: content/outputs/runs/Jan01_12-00-23_b6a73b3155bb/ (stored 0%)\n",
            "  adding: content/outputs/runs/Jan01_12-00-23_b6a73b3155bb/events.out.tfevents.1735732830.b6a73b3155bb.976.0 (deflated 62%)\n",
            "  adding: content/outputs/runs/Jan01_12-17-55_b6a73b3155bb/ (stored 0%)\n",
            "  adding: content/outputs/runs/Jan01_12-17-55_b6a73b3155bb/events.out.tfevents.1735733886.b6a73b3155bb.13975.0 (deflated 62%)\n",
            "  adding: content/outputs/runs/Jan01_12-24-13_b6a73b3155bb/ (stored 0%)\n",
            "  adding: content/outputs/runs/Jan01_12-24-13_b6a73b3155bb/events.out.tfevents.1735734263.b6a73b3155bb.13975.1 (deflated 65%)\n",
            "  adding: content/wandb/ (stored 0%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/ (stored 0%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/tmp/ (stored 0%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/run-or2bbu2y.wandb (deflated 82%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/files/ (stored 0%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/files/wandb-metadata.json (deflated 45%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/files/output.log (deflated 91%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/logs/ (stored 0%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/logs/debug-core.log (deflated 58%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/logs/debug-internal.log (deflated 82%)\n",
            "  adding: content/wandb/run-20250101_121808-or2bbu2y/logs/debug.log (deflated 82%)\n",
            "  adding: content/wandb/latest-run/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/tmp/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/run-or2bbu2y.wandb (deflated 82%)\n",
            "  adding: content/wandb/latest-run/files/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/latest-run/files/wandb-metadata.json (deflated 45%)\n",
            "  adding: content/wandb/latest-run/files/output.log (deflated 91%)\n",
            "  adding: content/wandb/latest-run/logs/ (stored 0%)\n",
            "  adding: content/wandb/latest-run/logs/debug-core.log (deflated 58%)\n",
            "  adding: content/wandb/latest-run/logs/debug-internal.log (deflated 82%)\n",
            "  adding: content/wandb/latest-run/logs/debug.log (deflated 82%)\n",
            "  adding: content/wandb/debug-internal.log (deflated 82%)\n",
            "  adding: content/wandb/debug.log (deflated 82%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaosB4exTl3-",
        "outputId": "5cebdb93-d482-49a6-9f2f-53a63124fb68"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXLUUaa_Y539"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}