{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mZMStRo51BKM"
      ],
      "authorship_tag": "ABX9TyNDm45LgsURBLnj2ppbvoh0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba3c43b4404c447cbb4b5a8e2d5b280a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11f8481455de483dbfbc2a2ad1b777b8",
              "IPY_MODEL_90946b9d5a32417faeebd6ab774f23e4",
              "IPY_MODEL_ee1b5eb3fb83486e9de62f78b9726b19"
            ],
            "layout": "IPY_MODEL_81e8ee453f404301b7c3bce9b227b85a"
          }
        },
        "11f8481455de483dbfbc2a2ad1b777b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd50723adba8494092f5fb54d3eff9bf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5a86a4a7b4b648c89b59707ce578eb2e",
            "value": "Mapâ€‡(num_proc=2):â€‡100%"
          }
        },
        "90946b9d5a32417faeebd6ab774f23e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0567ebb14fe3427dafc7f3264dffe7ce",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09ffb6e1f8064457b1c44abbd75780f5",
            "value": 12
          }
        },
        "ee1b5eb3fb83486e9de62f78b9726b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccac5dd8b69c4d1aa8b9a058d9e40481",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9beee3fe011245f696fa6a4012c1cfa8",
            "value": "â€‡12/12â€‡[00:02&lt;00:00,â€‡â€‡5.26â€‡examples/s]"
          }
        },
        "81e8ee453f404301b7c3bce9b227b85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd50723adba8494092f5fb54d3eff9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a86a4a7b4b648c89b59707ce578eb2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0567ebb14fe3427dafc7f3264dffe7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ffb6e1f8064457b1c44abbd75780f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccac5dd8b69c4d1aa8b9a058d9e40481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9beee3fe011245f696fa6a4012c1cfa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KENTAROSZK/unsloth_test/blob/main/unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTlImN1MHbCD",
        "outputId": "c55e30be-58bd-4fc9-dba8-1db8d96d60af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.12.12)\n",
            "Requirement already satisfied: unsloth_zoo>=2024.12.7 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2024.12.7)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.1+cu121)\n",
            "Requirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.0.29.post1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.0)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.9.5)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.47.1)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.2.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.14.0)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.27.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.1.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.12.7->unsloth) (24.12.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2024.12.7->unsloth) (11.0.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "# å‚è€ƒãƒšãƒ¼ã‚¸ï¼š\n",
        "### https://zenn.dev/the_exile/articles/unsloth-llama3-fine-tuning\n",
        "### https://qiita.com/Maki-HamarukiLab/items/891d03e570be9fc8d13b\n",
        "### https://qiita.com/Maki-HamarukiLab/items/a577d05cd224052378df\n",
        "\n",
        "!pip install unsloth\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## wandbã®è¨­å®š"
      ],
      "metadata": {
        "id": "_zPwmdoZxVf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "!wandb login $wandb_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAEKQbDVxU-w",
        "outputId": "6bd91edd-ac2f-4230-cfb2-c542ace77461"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™"
      ],
      "metadata": {
        "id": "_xSWmRFSxXTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# åŸºæœ¬è¨­å®š\n",
        "max_seq_length = 2048  # RoPEã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§è‡ªå‹•èª¿æ•´\n",
        "dtype = None  # GPUç¨®åˆ¥ã«å¿œã˜ã¦è‡ªå‹•é¸æŠ\n",
        "load_in_4bit = True  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã®4bité‡å­åŒ–\n",
        "\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhjL1OTuHfr8",
        "outputId": "b29b72dd-fa63-4765-ac03-120f26272752"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å‰ã«è³ªå•ã—ã¦ã¿ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "irUI1amgcTjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é«˜é€Ÿæ¨è«–ã®æœ‰åŠ¹åŒ–\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã‚’ç¶šã‘ã¦ãã ã•ã„: 1, 1, 2, 3, 5, 8,\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.5,\n",
        ")\n",
        "\n",
        "llm_answer = tokenizer.decode(\n",
        "    outputs[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPzRwOScXDC",
        "outputId": "1a29a9fd-8989-4b49-8f3d-332d99195852"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 01 Jan 2025\n",
            "\n",
            "user\n",
            "\n",
            "ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã‚’ç¶šã‘ã¦ãã ã•ã„: 1, 1, 2, 3, 5, 8,assistant\n",
            "\n",
            "ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã¯ã€å„é …ãŒå‰ã® 2 é …ã®åˆè¨ˆã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹æ•°åˆ—ã§ã™ã€‚æ¬¡ã®é …ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
            "\n",
            "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,...\n",
            "\n",
            "ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã¯ã€è‡ªç„¶æ•°ã®ç´ æ•°ã®åˆ†å¸ƒã‚„ã€è‡ªç„¶æ•°ã®å¹³æ–¹æ•°ã®æ•°ã®åˆ†å¸ƒãªã©ã€ã•ã¾ã–ã¾ãªæ•°å­¦çš„å•é¡Œã«ä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¿½åŠ \n",
        "\n",
        "åŠ¹ç‡çš„ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§å­¦ç¿’ã™ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "A25jdSQAhMfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¿½åŠ \n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªæ›´æ–°ã®ãŸã‚ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™ã€‚\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ©ãƒ³ã‚¯ï¼ˆæ¨å¥¨å€¤: 8, 16, 32, 64, 128ï¼‰\n",
        "    target_modules = [ # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹å‰Šé™¤ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ãŠå‹§ã‚ã—ã¾ã›ã‚“ã€‚ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 16, # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã€‚æ•°å€¤ãŒå¤§ãã„ã»ã©ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦å¤šãå­¦ç¿’ã—ã¾ã™ãŒã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’ä¿ƒé€²ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ©ãƒ³ã‚¯rã¨åŒã˜ã‹ã€2å€ã«ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
        "    lora_dropout = 0, # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€ã“ã‚Œã‚’0ã®ã¾ã¾ã«ã—ã¾ã™ã€‚ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’æ¸›ã‚‰ã™ã“ã¨ã¯ã§ãã¾ã™ãŒã€ãã‚Œã»ã©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "    bias = \"none\", # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€ã“ã‚Œã‚’0ã®ã¾ã¾ã«ã—ã¾ã™ã€‚\n",
        "    use_gradient_checkpointing = \"unsloth\", # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ã€Trueã€Falseã€\"unsloth\"ã§ã™ã€‚\"unsloth\"ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ã•ã‚‰ã«30%å‰Šæ¸›ã—ã€éå¸¸ã«é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€https://unsloth.ai/blog/long-context ã‚’ã”è¦§ãã ã•ã„ã€‚\n",
        "    random_state = 3407,\n",
        "    use_rslora=False,  # ãƒ©ãƒ³ã‚¯å®‰å®šåŒ–LoRAã‚’ã‚µãƒãƒ¼ãƒˆã€‚lora_alpha = 16ã‚’è‡ªå‹•çš„ã«è¨­å®šã™ã‚‹é«˜åº¦ãªæ©Ÿèƒ½ã§ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚\n",
        "    loftq_config = None,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvhch4XQHoGi",
        "outputId": "5794b3b4-268c-4bb2-84f6-36aff57b682c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"
      ],
      "metadata": {
        "id": "OSNIEaW-h5Am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ³¨æ„ï¼šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã«ã¯`dataset`å½¢å¼ã«ãªã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "8uvz36Hvykhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/the-exile-110/unsloth-llama3-fine-tuning/blob/main/dataset.json\n",
        "json_data =[\n",
        "  {\n",
        "    \"instruction\": \"å¿ƒè‡“ãŒä¸€ã¤ã ã‘æ®‹ã£ã¦ã‚‚ç”Ÿãã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"ã¯ã„ã€äººé–“ã¯ã‚‚ã¨ã‚‚ã¨å¿ƒè‡“ãŒä¸€ã¤ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"ã‚‚ã†ã™ãæ°´æ³³ã®æˆæ¥­ãŒã‚ã‚‹ã®ã«ã€æ˜¨æ—¥æ´—ã£ãŸæ°´ç€ãŒã¾ã ä¹¾ã„ã¦ã„ã¾ã›ã‚“ã€‚ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"æ°´æ³³ä¸­ã¯ã©ã†ã›æ°´ç€ãŒæ¿¡ã‚Œã‚‹ã®ã§ã€ä¹¾ã‹ã™å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"ä¸€åº¦ã—ã‹ç”Ÿã¾ã‚Œã¦ã„ãªã„ã®ã«ã€ãªãœæ¯å¹´èª•ç”Ÿæ—¥ã‚’ç¥ã†ã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"èª•ç”Ÿæ—¥ã‚’ç¥ã†ã®ã¯ã€ç”Ÿã¾ã‚ŒãŸæ—¥ã‚’ç¥ã†ã®ã§ã¯ãªãã€æ¯å¹´ã®æˆé•·ã¨çµŒé¨“ã‚’è¨˜å¿µã—ã€æœªæ¥ã®ä¸€å¹´ã«æœŸå¾…ã¨ç¥ç¦ã‚’è¾¼ã‚ã‚‹ãŸã‚ã§ã™ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"ç§ã¯25æ­³ã§çˆ¶ã¯50æ­³ã§ã™ã€‚ã¤ã¾ã‚Šã€çˆ¶ã¯2æ­³ã§ç§ã‚’ç”Ÿã‚“ã ã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"é•ã„ã¾ã™ã€‚ã‚ãªãŸãŒ25æ­³ã§ã€çˆ¶ãŒ50æ­³ãªã‚‰ã€çˆ¶ã¯ã‚ãªãŸãŒç”Ÿã¾ã‚ŒãŸã¨ã25æ­³ã ã£ãŸã¨ã„ã†ã“ã¨ã§ã™ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"5000å††ã§æ–°è»Šã‚’è²·ã†ãªã‚‰ä½•ãŒã„ã„ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"ã©ã†ã—ã¦ã‚‚ç­”ãˆã‚‹ãªã‚‰ã€ãŠã‚‚ã¡ã‚ƒã®è»Šã—ã‹è²·ãˆãªã„ã§ã—ã‚‡ã†ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"é…¸ç´ ã¨æ°´ç´ ã‚’åŒæ™‚ã«å¸å…¥ã—ãŸã‚‰ã€æ°´ã‚’é£²ã‚“ã§ã„ã‚‹ã“ã¨ã«ãªã‚‹ã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"é•ã„ã¾ã™ã€‚æ°´ã¯æ°´ç´ ã¨é…¸ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ãŒã€å˜ã«é…¸ç´ ã¨æ°´ç´ ã‚’å¸å…¥ã™ã‚‹ã ã‘ã§ã¯æ°´ã¯å½¢æˆã•ã‚Œã¾ã›ã‚“ã€‚æ°´ã®å½¢æˆã«ã¯ç‰¹å®šã®æ¡ä»¶ä¸‹ã§åŒ–å­¦åå¿œãŒå¿…è¦ã§ã€ãã®åå¿œã¯æ¿€ã—ãã€å¤§é‡ã®ç†±ã‚’ç™ºç”Ÿã—ã¾ã™ã€‚äººä½“ã«ã¨ã£ã¦éå¸¸ã«å±é™ºã§ã™ã®ã§ã€é…¸ç´ ã¨æ°´ç´ ã‚’å¸å…¥ã—ã¦æ°´ã‚’é£²ã‚€ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"èµ¤å£ã®æˆ¦ã„ã§ã€æ›¹æ“ã¯ãªãœ119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ãªã‹ã£ãŸã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"ä¸‰å›½æ™‚ä»£ã«ã¯é›»è©±ã‚„ç¾ä»£ã®æ¶ˆé˜²ã‚·ã‚¹ãƒ†ãƒ ãŒãªã‹ã£ãŸã®ã§ã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"ä¸€æ˜¨æ—¥ã¯èª•ç”Ÿæ—¥ãŠã‚ã§ã¨ã†ã¨è¨€ã‚ã‚ŒãŸã®ã«ã€ä»Šæ—¥ã¯ãªãœè¨€ã‚ã‚Œãªã„ã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"ä»Šæ—¥ã¯ã‚ãªãŸã®èª•ç”Ÿæ—¥ã§ã¯ãªãã€ä¸€æ˜¨æ—¥ãŒã‚ãªãŸã®èª•ç”Ÿæ—¥ã ã£ãŸã‹ã‚‰ã§ã™ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"Bluetoothã‚¤ãƒ¤ãƒ›ãƒ³ãŒå£Šã‚ŒãŸã®ã§ã™ãŒã€ç—…é™¢ã®æ­¯ç§‘ã‹è€³ç§‘ã«è¡Œãã¹ãã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"Bluetoothã‚¤ãƒ¤ãƒ›ãƒ³ãŒå£Šã‚ŒãŸå ´åˆã€ä¿®ç†åº—ã‚„å°‚é–€ã®é›»å­è£½å“ä¿®ç†æŠ€è¡“è€…ã«é ¼ã‚€ã¹ãã§ã€ç—…é™¢ã®æ­¯ç§‘ã‚„è€³ç§‘ã«è¡Œãã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"1ã‚­ãƒ­ã®ç¶¿ã¨1ã‚­ãƒ­ã®é‰„ãŒåŒæ™‚ã«æ°´ã«è½ã¡ãŸã‚‰ã€ã©ã¡ã‚‰ã‚’å…ˆã«æ•‘ã„ã¾ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"å®Ÿéš›ã«ã¯ã€é‰„ã¯æ°´ã«æ²ˆã¿ã€ç¶¿ã¯æ°´é¢ã«æµ®ãã®ã§ã€æ•‘ã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã©ã¡ã‚‰ã‚‚æ•‘ã†å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"åˆ‘å‹™æ‰€ã«ã¯å…¨ã¦ç½ªäººãŒã„ã‚‹ã®ã«ã€ãªãœè­¦å¯Ÿã¯åˆ‘å‹™æ‰€ã«è¡Œã£ã¦æ•ã¾ãˆãªã„ã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"å½¼ã‚‰ã¯ã™ã§ã«æ•ã¾ã£ã¦åˆ‘å‹™æ‰€ã«ã„ã‚‹ã®ã§ã€å†ã³æ•ã¾ãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "  },\n",
        "  {\n",
        "    \"instruction\": \"äººé–“ã«ã¯å·¦è€³ã¨å³è€³ã—ã‹ãªã„ã®ã«ã€ãªãœä¸­è€³ç‚ã«ãªã‚‹ã®ã§ã™ã‹ï¼Ÿ\",\n",
        "    \"input\": \"\",\n",
        "    \"output\": \"ä¸­è€³ç‚ã¯ã€ŒçœŸã‚“ä¸­ã®è€³ã€ã«ç™ºç”Ÿã™ã‚‹ã®ã§ã¯ãªãã€è€³ã®æ§‹é€ ã®ä¸­è€³éƒ¨åˆ†ã«ç™ºç”Ÿã™ã‚‹ç‚ç—‡ã§ã™ã€‚è€³ã®æ§‹é€ ã¯å¤–è€³ã€ä¸­è€³ã€å†…è€³ã®ä¸‰ã¤ã«åˆ†ã‹ã‚Œã¦ãŠã‚Šã€ä¸­è€³ç‚ã¯ä¸­è€³éƒ¨åˆ†ãŒç‚ç—‡ã‚’èµ·ã“ã™ã“ã¨ã§ã™ã€‚ã¤ã¾ã‚Šã€äººé–“ã«ã¯å·¦è€³ã¨å³è€³ã—ã‹ãªãã¦ã‚‚ä¸­è€³ç‚ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "werIX708H38H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ™‚ç”¨ã«datasetã‚’ä½œæˆã™ã‚‹\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”¨æ„ã™ã‚‹\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "def formatting_prompt(examples: list) -> dict:\n",
        "    instructions = [item[\"instruction\"] for item in examples]\n",
        "    inputs = [item[\"input\"] for item in examples]\n",
        "    outputs = [item[\"output\"] for item in examples]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "_dataset = formatting_prompt(json_data)\n",
        "# datasetã®å‹\n",
        "dataset = Dataset.from_dict(_dataset)"
      ],
      "metadata": {
        "id": "u2Qpvs0ZoQSN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rWyasXriieN",
        "outputId": "04eecee5-1082-4225-e5f3-c38e0ac48e69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nå¿ƒè‡“ãŒä¸€ã¤ã ã‘æ®‹ã£ã¦ã‚‚ç”Ÿãã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ\\n### Input:\\n\\n### Response:\\nã¯ã„ã€äººé–“ã¯ã‚‚ã¨ã‚‚ã¨å¿ƒè‡“ãŒä¸€ã¤ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"
      ],
      "metadata": {
        "id": "TYyFUZ0Eh874"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,  # çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å ´åˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’5å€é«˜é€ŸåŒ–ã§ãã¾ã™\n",
        "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,# GPUã®ãƒ¡ãƒ¢ãƒªã‚’ã‚ˆã‚Šå¤šãæ´»ç”¨ã—ãŸã„å ´åˆã¯ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã—ã¾ã™ã€‚ã¾ãŸã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚ˆã‚Šã‚¹ãƒ ãƒ¼ã‚ºã«ã—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã‚’é˜²ããŸã‚ã«ã‚‚ã€ã“ã‚Œã‚’å¢—ã‚„ã—ã¾ã™ã€‚ãŸã ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Ÿéš›ã«é…ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€é€šå¸¸ã¯ãŠå‹§ã‚ã—ã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«ã€gradient_accumulation_stepsã‚’å¢—ã‚„ã™ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
        "        gradient_accumulation_steps = 4,#ä¸Šè¨˜ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ã“ã¨ã¨åŒã˜åŠ¹æœãŒã‚ã‚Šã¾ã™ãŒã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»é‡ã«ã¯å½±éŸ¿ã—ã¾ã›ã‚“ã€‚é€šå¸¸ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±æ›²ç·šã‚’ã‚ˆã‚Šã‚¹ãƒ ãƒ¼ã‚ºã«ã—ãŸã„å ´åˆã¯ã€ã“ã‚Œã‚’å¢—ã‚„ã™ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€ã‚¹ãƒ†ãƒƒãƒ—ã‚’60ã«è¨­å®šã—ã¾ã—ãŸã€‚æ•°æ™‚é–“ã‹ã‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã®å ´åˆã¯ã€ä»£ã‚ã‚Šã«max_stepsã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã€num_train_epochs = 1ã«ç½®ãæ›ãˆã¾ã™ã€‚1ã«è¨­å®šã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’1å›å®Œå…¨ã«ãƒ‘ã‚¹ã—ã¾ã™ã€‚é€šå¸¸ã¯1ã€œ3ãƒ‘ã‚¹ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ãã‚Œä»¥ä¸Šã¯æ¨å¥¨ã—ã¾ã›ã‚“ã€‚ãã‚Œä»¥ä¸Šãƒ‘ã‚¹ã™ã‚‹ã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã®å¯èƒ½æ€§ãŒé«˜ããªã‚Šã¾ã™ã€‚\n",
        "        #num_train_epochs=1\n",
        "        learning_rate = 2e-4, # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’é…ãã—ãŸã„å ´åˆã¯å­¦ç¿’ç‡ã‚’ä¸‹ã’ã¾ã™ãŒã€ã‚ˆã‚Šé«˜ã„ç²¾åº¦ã®çµæœã«åæŸã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚é€šå¸¸ã¯ã€è©¦ã—ã¦ã¿ã‚‹å€¤ã¨ã—ã¦ã€2e-4ã€1e-4ã€5e-5ã€2e-5ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# å­¦ç¿’ã®å®Ÿè¡Œ\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba3c43b4404c447cbb4b5a8e2d5b280a",
            "11f8481455de483dbfbc2a2ad1b777b8",
            "90946b9d5a32417faeebd6ab774f23e4",
            "ee1b5eb3fb83486e9de62f78b9726b19",
            "81e8ee453f404301b7c3bce9b227b85a",
            "cd50723adba8494092f5fb54d3eff9bf",
            "5a86a4a7b4b648c89b59707ce578eb2e",
            "0567ebb14fe3427dafc7f3264dffe7ce",
            "09ffb6e1f8064457b1c44abbd75780f5",
            "ccac5dd8b69c4d1aa8b9a058d9e40481",
            "9beee3fe011245f696fa6a4012c1cfa8"
          ]
        },
        "id": "oSzlqxPFiBsD",
        "outputId": "4cec66ef-a67b-44bf-ddc9-41ce70437b17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/12 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba3c43b4404c447cbb4b5a8e2d5b280a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 12 | Num Epochs = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 02:04, Epoch 30/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.858900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.192700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.877100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.103700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.776300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.311100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.124900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.372400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.139100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.018900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.012600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.741100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.872400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.631700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.524700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.502100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.391100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.324900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.285300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.254200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.230400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.210800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.181700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.170700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.153000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.144900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.141800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.137300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.113200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.143400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.103000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.097500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.093800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.085400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.088900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.077800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.077800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.080400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.083600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.058900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.066800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.071900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.065100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.063600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.056400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.075200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.060100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.057600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦æ¨è«–ã•ã›ã‚‹"
      ],
      "metadata": {
        "id": "7wmzlT8By2Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é«˜é€Ÿæ¨è«–ã®æœ‰åŠ¹åŒ–\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã‚’ç¶šã‘ã¦ãã ã•ã„: 1, 1, 2, 3, 5, 8,\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "llm_answer = tokenizer.decode(\n",
        "    outputs[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "id": "gEs-kUGxH4BA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4b6bc5-2253-4677-e0de-5722e05e15ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 01 Jan 2025\n",
            "\n",
            "user\n",
            "\n",
            "ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã‚’ç¶šã‘ã¦ãã ã•ã„: 1, 1, 2, 3, 5, 8,assistant\n",
            "\n",
            "æ¬¡ã®æ•°ã¯8ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã«ã¯æ¬¡ã®æ•°ã¯8ã¨è¨€ã‚ã‚Œãªã„ã§ã€æ¬¡ã®æ•°ã¯8ã¨åŒæ™‚ã«ç¾ã‚Œã‚‹ã®ã§ã¯ãªãã€æ¬¡ã®æ•°ã¯8ã¨åŒæ™‚ã«ç¾ã‚Œã‚‹ã®ã§ã¯ãªãã€æ¬¡ã®æ•°ã¯8ã¨åŒæ™‚ã«ç¾ã‚Œã‚‹ã®ã§ã¯ãªãã€æ¬¡ã®æ•°ã¯8ã®æ¬¡ã«ç¾ã‚Œã‚‹æ•°ã§ã€å®Ÿéš›ã«ã¯9ã§ã™ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ­£ã—ãè¨€ãˆã°ã€1ã€1ã€2ã€3ã€5ã€8ã€9ã§ã™ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# é«˜é€Ÿæ¨è«–ã®æœ‰åŠ¹åŒ–\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"èµ¤å£ã®æˆ¦ã„ã§ã€æ›¹æ“ã¯ãªãœ119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ãªã‹ã£ãŸã®ã§ã™ã‹ï¼Ÿ\"\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    max_new_tokens=1024,\n",
        "    temperature=0.3,\n",
        ")\n",
        "\n",
        "llm_answer = tokenizer.decode(\n",
        "    outputs[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "\n",
        "print(llm_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISbX-ufJQtjC",
        "outputId": "40326dda-96bd-43e4-d38d-c44da5f90520"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 01 Jan 2025\n",
            "\n",
            "user\n",
            "\n",
            "èµ¤å£ã®æˆ¦ã„ã§ã€æ›¹æ“ã¯ãªãœ119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ãªã‹ã£ãŸã®ã§ã™ã‹ï¼Ÿassistant\n",
            "\n",
            "æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚119ã¯ç¾ä»£ã®æ¶ˆé˜²ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚Šã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã“ã¨ã§ã™ã€‚æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã®ã§ã€èµ¤å£ã®æˆ¦ã„ã§ã©ã†ã—ãŸã‚‰ã„ã„ã§ã™ã‹ï¼Ÿ\n",
            "\n",
            "å®Ÿéš›ã«ã¯ã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€ç¾ä»£ã®æ¶ˆé˜²ã‚·ã‚¹ãƒ†ãƒ ãŒãªã‹ã£ãŸã®ã§ã€æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã«ã¯ã©ã†ã—ãŸã‚‰ã„ã„ã§ã™ã‹ï¼Ÿ\n",
            "\n",
            "æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã“ã¨ã§ã™ã€‚ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "å®Ÿéš›ã«ã¯ã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã“ã¨ã§ã™ã€‚ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119ã«é›»è©±ã—ã¦æ¶ˆé˜²ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã€119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
            "\n",
            "ã¤ã¾ã‚Šã€æ›¹æ“ã¯119ã«é›»è©±ã—ã¦æ•‘åŠ©ã‚’æ±‚ã‚ã‚‹ã®ã§ã¯ãªãã€119ã¯119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹"
      ],
      "metadata": {
        "id": "mZMStRo51BKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "IH7iMvVMH4JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21acdaa-b646-42fb-fc89-35fc8dbcb6ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«ã‚’çµåˆã—ã€GGUFé‡å­åŒ–ã—ã¦ä¿å­˜ã™ã‚‹"
      ],
      "metadata": {
        "id": "_DRnDvIH1F86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_gguf(\n",
        "    \"model\",\n",
        "    tokenizer,\n",
        "    quantization_method = [\n",
        "        \"q4_k_m\",\n",
        "        \"q8_0\",\n",
        "        \"q5_k_m\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "uAN4Ru4yH4Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcbfccf-80dd-44a5-937c-4670a3320741"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Will remove a cached repo with size 1.4K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.34 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:01<00:00, 22.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving model/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at model into f16 GGUF format.\n",
            "The output location will be /content/model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 24\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-01-01 12:58:55.295776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-01 12:58:55.333674: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-01 12:58:55.340968: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-01 12:58:57.537822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- if strftime_now is defined %}\n",
            "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "    {%- else %}\n",
            "        {%- set date_string = \"26 Jul 2024\" %}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "        {{- '\"parameters\": ' }}\n",
            "        {{- tool_call.arguments | tojson }}\n",
            "        {{- \"}\" }}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
            "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.43G/6.43G [01:33<00:00, 69.0Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4404 (0827b2c1)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  1918.35 MB\n",
            "\n",
            "main: quantize time = 342033.74 ms\n",
            "main:    total time = 342033.74 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
            "main: build = 4404 (0827b2c1)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q8_0.gguf' as Q8_0 using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q8_0 .. size =   751.50 MiB ->   399.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  3255.90 MB\n",
            "\n",
            "main: quantize time = 80100.95 ms\n",
            "main:    total time = 80100.95 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q8_0.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
            "main: build = 4404 (0827b2c1)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q5_K_M.gguf' as Q5_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  2207.10 MB\n",
            "\n",
            "main: quantize time = 302381.78 ms\n",
            "main:    total time = 302381.78 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q5_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16bitå½¢å¼ã§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "# VLLMãªã©ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯16bitå½¢å¼ãŒä¾¿åˆ©ã§ã™ã€‚\n",
        "model.push_to_hub_merged(\n",
        "    \"./model-name-16bit\",\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "JNwl7O4_1Tnt",
        "outputId": "ff290cf3-6637-4fa1-a419-d77c2fdede27"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unsloth: Please supply a token!\nGo to https://huggingface.co/settings/tokens",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLocalTokenNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0;34mf\"{self.endpoint}/api/whoami-v2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m             headers=self._build_hf_headers(\n\u001b[0m\u001b[1;32m   1627\u001b[0m                 \u001b[0;31m# If `token` is provided and not `None`, it will be used by default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_build_hf_headers\u001b[0;34m(self, token, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   9089\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9090\u001b[0;31m         return build_hf_headers(\n\u001b[0m\u001b[1;32m   9091\u001b[0m             \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, library_name, library_version, user_agent, headers, is_write_action)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# Get auth token to send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mtoken_to_send\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_token_to_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_headers.py\u001b[0m in \u001b[0;36mget_token_to_send\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             raise LocalTokenNotFoundError(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;34m\"Token is required (`token=True`), but no token found. You\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLocalTokenNotFoundError\u001b[0m: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-905f9be7575b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 16bitå½¢å¼ã§ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# VLLMãªã©ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯16bitå½¢å¼ãŒä¾¿åˆ©ã§ã™ã€‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model.push_to_hub_merged(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"./model-name-16bit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_push_to_hub_merged\u001b[0;34m(self, repo_id, tokenizer, save_method, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m     \u001b[0munsloth_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    261\u001b[0m                 \u001b[0;34m\"Unsloth: Please supply a token!\\n\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;34m\"Go to https://huggingface.co/settings/tokens\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: Please supply a token!\nGo to https://huggingface.co/settings/tokens"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvyEVGHc1w5F",
        "outputId": "9cb807be-8c62-4843-9a13-4f44eacc53ec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaQCQGYwQIgd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}